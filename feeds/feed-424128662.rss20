<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="http://www.pbs.org/cringely/pulpit/rss/pulpit.rss.xsl"?>
<rss version="2.0">
	<channel>
		<title>I, Cringely . The Pulpit | PBS</title>
		<link>http://www.pbs.org/cringely/pulpit/</link>
		<description>I, Cringely is the blog of Robert X. Cringely. Copyright 2006 PBS Online.</description>
		<language>en</language>
		<copyright>Copyright 2008 PBS Online</copyright>
		<lastBuildDate>Sun, 07 Dec 2008 09:05:13 -0500</lastBuildDate>
		<docs>http://blogs.law.harvard.edu/tech/rss</docs> 

				<item>
			<title>Insanely Great</title>
			<description><![CDATA[<p>Looking for improved business models for the personal computer business, Apple CEO Steve Jobs often used to cite automobile makers, though never American car companies.  The examples were invariably German. Whether it was the design aesthetic of his Mercedes sedan or Porsche's success at selling high-margin cars as entertainment devices, Jobs could always point to farfegnugen as a way to sell a good car for a great price.  So since he thinks about these things anyway, and because the U.S. automobile industry is on the skids and begging for help this week, I find myself wondering what would happen if Steve Jobs were put in charge of any of the Big Three car companies?</p>

<p>It wouldn't be boring, that's for sure, and I'm fairly certain Steve could do a better job than the Detroit executives currently in charge.</p>

<p>When Steve Jobs returned to Apple in 1997, the computer company was in worse shape than some of these car companies. Apple's share price was in the toilet, it had poorly conceived products it couldn't sell, the company was losing money, market share was dismal, and CEOs from John Sculley on had tried without success to find ANY company that would buy Apple.  Steve himself had such low expectations for Apple under Gil Amelio that he sold all his new Apple shares shortly after Apple bought his NeXT Computer.</p>

<p>What a difference a decade makes.  Today Apple and Jobs are at the top of their game, taking market share from other computer companies while at the same time establishing game-changing new product concepts like the iPod and iPhone.  Apple is America's largest music seller (who could have seen that one coming back in '97? Nobody), has no debt, and $22+ billion in the bank.  Even at its currently depressed stock price, Apple is worth more than any of the car companies and for good reason: Apple has a future.</p>

<p>What did Jobs do to make Apple such a business success and how would he translate these techniques to a car company?  It's not really that hard to imagine.</p>

<p>Back in 1997 Apple had a huge list of products it made or sold, many of them not for a profit.  Here is a partial list of Apple products from 1997 courtesy of my friend Orrin, who brought this idea to my attention:</p>

<p>PowerBook<br>
Quadra<br>
Performa<br>
Power Macintosh<br>
Workgroup and network servers
LaserWriter laser printers<br>
StyleWriter inkjet printers<br>
Newton PDAs<br>
Displays<br>
External disk drives<br>
Modems<br>
Scanners<br>
Lots of software</p>

<p>And don't forget the Mac clones.  Jobs killed the clones, dropped the Newton, and streamlined the Mac product line into what today are four ranges of computers -- personal and professional, desktop and portable.  Yes, there are the Mac Mini and the xServe, I know, but nearly all Apple computer sales lie with the MacBooks, MacBook Pros, iMacs and Mac Pros.</p>

<p>Apple quit the printer business entirely and, over time, got out of the business of manufacturing its own computers at all.</p>

<p>The decisions Steve Jobs made in 1997 were that Apple's core competence was in making computers and its future then lay with graphics and desktop publishing professionals who loved the products.  While these conclusions may seem obvious, they weren't reflected in the Apple product line at the time.  Steve knew the value he had in his product development team, too, which was a clear difference between he and Sculley, Spindler, and Amelio, all of whom had come in varying degrees under the sway of the diabolical product development chief Jean-Louis Gassee.</p>

<p>One advantage of my having written about this industry since dinosaurs roamed the earth is that there are columns about Apple in my archive dating from 1997 that give a sense of what the company, its products and lack of leadership were like at the time.  Read them: they are in this week's links.  They give a sobering look at how bad things were and show an eery resemblance to the positions of the automakers today.</p>

<p>Look at the American car companies with their many brands that often compete with each other within a single company.  It's bad enough competing with Chrysler and GM, but why should Ford be competing with itself?  There has been some streamlining over the years (goodbye Plymouth and Oldsmobile) but not enough.  There are simply too many models chasing too few buyers.  So long Mercury.</p>

<p>The first lesson Jobs learned was that he couldn't build a successful company selling products at a loss.  While we can argue that Apple prices are higher than they might be, nobody can argue with Apple's quality or its success at selling those products.  So the first thing Jobs would do as head of a U.S. car company would be to eliminate the lines that are showing  -- and have long shown -- little or no profit, which today generally means the biggest and the smallest cars. Goodbye Hummer.</p>

<p>Honda is an archetype for this sort of marketing, having a limited line of cars with nothing down at the bottom fighting it out with Kia and Hyundai.  A Honda Fit may be inexpensive but it isn't cheap.</p>

<p>There is a lot of conventional wisdom at work in the car business and some of it is completely outmoded.  Why, for example, is it so important to have a complete line of cars for every customer age and financial circumstance?  That made good sense at a time when America was being introduced to car ownership and a brand could grow with its customers as their financial circumstances and taste in cars changed over time.  But the car market is beyond mature today and doing things primarily because it made sense to do so in the era of Henry Ford and Alfred Sloan, well that makes no sense at all.</p>

<p>The business press loves to differentiate between two types of auto executives -- the financial types typified by GM CEO Rick Wagoner and the "car guys" personified by GM vice chairman Bob Lutz (who also did stints at Chrysler and Ford).  When the companies periodically lose their way, it's attributed to too much finance and not enough car.  But Steve Jobs is something in-between.  No large American company in any industry has tighter financial controls than Apple, yet the strength of Apple is supposed to be its design.  All this proves is that the finance-versus-car-guy scenario loved by Fortune and Forbes is simply bogus.</p>

<p>It's not that there aren't smart executives at these car companies, but they are shackled with several bad ideas and exist in an unrealistic corporate environment.</p>

<p>Their main delusion is the myth of the complete car line.  Apple in 1997 had a tremendous advantage in being clearly a minority player.  There was no hope that the Mac OS would topple Windows, but that made chipping away at Windows a tactical effort where significant advances could be made by Apple just concentrating on niche markets.  The U.S. automobile makers can't (or won't) do that because again they think they have to make every type of car for every type of buyer.  Yet each company IS a minority player; they just pretend that this condition is temporary, but it isn't.</p>

<p>This corporate delusion of majority status has meant that it simply wasn't possible for any of the car companies to take truly radical actions.  They can't take big risks on new technology because the downside is perceived as being too big.  Yet the effect of this over time has been to virtually guarantee that downside as the companies die from inaction or, more properly, UNDER action.</p>

<p>That's where Steve Jobs' second strength comes into play -- identifying important new technologies.  He'd look at the car market and conclude a number of things: 1) it's a no-brainer to embrace dramatic design (no boring cars); 2) performance sells, and; 3) safety and fuel economy are co-equal secondary goals.  So Steve's goal for his car company would be to make a limited line of vehicles that were dramatically styled with visibly different technologies from the competitors and were uniformly 20+ percent safer and 20+ percent more fuel-efficient.</p>

<p>That's not so hard to do, either, as I showed last week with my DA-2A example.  Or look at XP Vehicles, the company that will sell you an inflatable car that arrives at your house in a box.  But embracing these ideas requires the companies do something else that Jobs came to embrace with Apple's products - stop building most of their own cars.</p>

<p>There are two aspects to this possible outsourcing issue.  First is the whole concept of car companies as manufacturing their own products.  There is plenty of outsourcing of car components.  Most companies don't make their own brakes, for example.  Yamaha makes whole engines for Ford.  Entire model lines are bought and rebadged from one maker to another.  But nobody does it for everything, yet that's what Steve Jobs would do.</p>

<p>All the U.S. car companies are closing plants, for example, and all are doing so because of overcapacity.  But what would happen if just one of those companies -- say Chrysler -- decided that two years from now it would no longer actually assemble ANY of its own vehicles?  Instead they'd put out an RFQ to every company in the world for 300,000 Chrysler Town & Country minivans as an example.  Now THAT would be a dramatic move.</p>

<p>And a good one, frankly, because with a single pen stroke most of the overcapacity would be removed from the U.S. car market.  Chrysler would have to shut down all those plants and lay off all those people, true, but doing it all the way all at once would change the nature of the company's labor agreements such that there wouldn't be a whimper.  When you are eliminating 8 percent of capacity the tussle is over WHICH 8 percent.  When you are eliminating ALL capacity, there is no tussle.</p>

<p>So Chrysler reaches out to contract manufacturers in this scenario and you know those manufacturers would fight for the work and probably give Chrysler a heck of a deal.  For current models, for example, Chrysler could probably sell the tooling and maybe even the entire assembly plant for a lot more than they'd get from the real estate alone.  But that particular advantage, I'd say, would be unique to the first big player to throw in the production towel.</p>

<p>In this scenario, Chrysler becomes a design, marketing, sales, and service organization.  What's wrong with that?  They can change products more often and more completely because of their dramatically lower investment in production capital.  They can pit their various suppliers against each other more effectively than could a surviving car manufacturer.  It's what Steve would do.</p>

<p>And Steve would also embrace one dramatic new technology, whether it is electric, hydrogen, natural gas, whatever, but he'd do it in a very Steveian fashion, which is to say exactly the way he did the iPod and iTunes.  That is, he'd sell you the car and then sell you whatever is required to fill up the car.  This has always been a barrier for the car companies because they couldn't imagine themselves in the business of running electric/hydrogen/LPG stations, while Steve would imagine his company MAKING A PROFIT running just those stations.</p>

<p>Steve would take an existing operation that already had an ideal geographic distribution like McDonald's restaurants.  He buy McDonald's or seduce the company into a deal.  Then he'd embrace a propulsion technology like advanced electric capacitors -- batteries that could be recharged in less than a minute -- and put charging stations on the drive-through lanes.  By the time the electric models were ready for sale he'd have 12,000 charging stations in place to serve them.  Would you like fries with that charge?  </p>

<p>Is it too late for the Big Three? Ford is the strongest company from what I've seen, but I believe there may be some creative juices in GM, too. Their prototype car the Volt takes hybrid cars to the next level, I just wish they were selling them now because Toyota or Honda will probably beat them to the market with something similar. Another thing Apple does well is product introductions. They very rarely show their hand before they are ready to send you home with one.  GM announced the Volt in January of 2007 yet it is still slated for sale by 2011. </p>

<p>Stupid.</p>
 
]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081207_005508.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081207_005508.html</guid>
			<category></category>
			<pubDate>Sun, 07 Dec 2008 09:05:13 -0500</pubDate>
		</item>
				<item>
			<title>Saving Detroit</title>
			<description><![CDATA[<p>My first car was an Oldsmobile, a red 1966 convertible I wish I still owned today.  It was big and heavy yet somehow managed to average 18 miles per gallon in an era when gasoline cost 35 cents.  Detroit and the U.S. automakers ruled the world when that car was built, yet now the companies say they are on the skids, bleeding money and headed for bankruptcy.  What happened?  And what can we do -- if anything -- to save an industry that for a century defined our nation as well as our youth?  I have some ideas.</p>

<p>Whatever the mechanism of their demise, the car companies did it to themselves.  They love to blame labor agreements, pension plans, and health plans for their precarious financial situation, yet didn't the companies negotiate and sign those deals in good faith?  Surely the down-the-road financial burdens were calculable at the time.  Is it that we're living longer than expected, rather than expiring early like Pinto gas tanks? Maybe that's part of it, but to blame the unions for good negotiating is worse than forgiving the companies for bad.  And what does it matter?  The real issue at hand -- and the only one that really matters -- isn't who to blame or even whether or not to save these specific companies, but how to get me a really sweet ride.  That's because the only way the U.S. auto industry is going to survive in any form is by making cars so cool that we'll stand in line to buy them even in a global financial crisis.</p>

<p>It's the cars, stupid.</p>

<p>My hobby is building small airplanes and one of my favorites is a Davis DA-2A, winner of the Outstanding New Design contest in 1966, the same year my Oldsmobile (and my current Thunderbird convertible) was built. That little Davis can teach us a lot about cars.</p>

<p>I didn't build my DA-2A, but I am rebuilding it right now and know it intimately.  My Davis is an all-aluminum two-seater with an 85-horsepower engine.  The engine was built in 1946, the plane in 1982, and the whole thing cost under $4,000 at the time, though today I have more than that invested in the instrument panel alone.  The plane weighs 625 lbs. empty, 1125 lbs. loaded, has a top speed of 140 miles per hour and can travel about 600 miles on its 24-gallon fuel tank.</p>

<p>Why can't I buy a car like that?</p>

<p>Imagine if we took the basic design parameters of my DA-2A and applied them to a modern automobile.  The new design would have to carry two people and luggage, have an empty weight of no more than 625 lbs. and use an 85-horsepower engine.  With a loaded weight of 1125 lbs., the car would have a power-to-weight ratio comparable to a Chevy Corvette and be just as quick -- probably even faster than the airplane's 140 mph.  Driven only 20 percent over posted speed limits as God intended, the car would easily get 50+ miles per gallon.</p>

<p>Who wouldn't want to buy one?</p>

<p>At the heart of manufacturing is the simple concept of buying raw materials in volume at a low price per pound and selling manufactured products at retail for a high price per pound.  The eventual retail price per pound is determined by the marketplace and ideally it ought to be high enough for the manufacturer to make a profit.  The very light weight of our DA-2A car analog suggests that it ought to be inexpensive to buy, but maybe all that means is we have to look beyond the car industry to bicycles.</p>

<p>Car buyers and bicycle buyers approach retail pricing from completely different directions.  Car buyers, whether they think about it this way or not, traditionally try to buy cars that cost the least on a per-pound basis.  Do some research on the Internet and you'll see that luxury cars, whether we are talking about a Cadillac SUV or a big Mercedes sedan, tend to cost about $10 per pound; mid-range cars cost about $6 per pound; and economy cars cost about $4 per pound.  Manufacturers prefer luxury cars because, given the same profit margins, they make vastly more gross profit on a fancy car than they do on an entry-level car.  This pricing bias is part of what is working against Detroit right now.</p>

<p>Bicycles are different.  Bicycle buyers, whether they are conscious of their behavior or not, try to pay the MOST per pound rather than the least.  A lighter bike is always a better bike and a more expensive bike.  Cheap bikes from Wal-Mart tend to cost about $2 per pound, nice bikes from a bike shop cost about $20 per pound, and top-of-the-line racing bikes cost about $200 per pound which, interestingly, is about the same per-pound cost as a top-of-the-line Ferrari or Aston-Martin.</p>

<p>So the trick to turning around the U.S. auto industry is to make car buyers adopt the values of bicycle buyers, which implies the willingness to pay $20 per pound of final product.  The way to achieve that goal is by building cars that are both affordable at $20 per pound and EXCITING TO DRIVE.</p>

<p>Under this formula, the car version of my DA-2A would cost $12,500, making it broadly affordable.  Yet with 6061 aluminum alloy selling in volume for around $1.60 per pound, there ought to be plenty of profit in there for the companies.</p>

<p>Detroit doesn't understand that.</p>

<p>Just as the price point bias tends to push manufacturers toward heavier cars, so do consumer buying habits and even government regulations.  Trucks overtook cars in the 1990s as America's most popular vehicles and that wasn't some grand plan from General Motors or Ford, it just happened.  The companies were grateful of course -- ecstatic even -- because trucks were already profitable on commercial sales alone so the consumer truck boom came with almost no additional fixed costs.  Trucks were INCREDIBLY profitable and heavy.</p>

<p>So were SUVs.  When I was a kid there were Chevy Suburbans and Jeep Cherokees, but I didn't know anyone who owned one.  SUVs grew out of the truck boom and were yet another Detroit windfall, this time finding a way to charge $10 per pound for a truck. Wow!</p>

<p>Government regulations began pushing car companies down the path of inefficiency with the passage of the Clean Air Act in 1967.  Cleaning the air was a legitimate goal but the way Detroit went about complying was not.  First they installed air pumps to force complete combustion of exhaust gases IN THE EXHAUST, not in the engine where power could be produced.  To make this work reliably they had to richen the fuel mixture to ensure that there was enough unburned gasoline in the exhaust to burn with the air introduced by the air pump.</p>

<p>Am I the only one who sees problems with this approach?  To lower exhaust emissions reliably over the average 100,000-mile life expectancy of a car, the companies deliberately used more gas, hurting gas mileage.  What did they care, right?  Gas was 35 cents per gallon.  But the companies were already on a slippery slope. </p>

<p>In 1972 the companies were forced to reduce compression ratios to accommodate unleaded fuel.  Again the reason was laudable but the reaction was not.  The lower-compression engines were less efficient, so to get performance back you had to buy a bigger engine -- paying the same amount per pound but buying more pounds.  Detroit liked that.</p>

<p>Catalytic converters came along in 1975, and again required richening the fuel-air mixture for proper operation over the 100,000-mile vehicle life.</p>

<p>I'm not arguing here against environmental regulations but against the way they are frequently applied.  This happens in other fields, too.  Your cardiologist will recommend barbequing to reduce fat while your oncologist prefers frying to reduce carcinogen exposure.  Either way you are still going to die.</p>

<p>For 40 years we've had a succession of slight product modifications to accommodate new automotive regulations in generally the wrong ways.  The car companies fought against airbags because of the cost, not because of any safety issue.  Their safety bias was always toward the heavier vehicle even though statistics show big SUVs are actually less safe than smaller cars.  This was simply because they wanted to sell more pounds of car to make more money.</p>

<p>Technical innovations are a hard sell in Detroit because most of them fail.  Even my idea of a light weight yet powerful car was tried before in the Crosley Hotshot of the late 1940s -- reason enough, many car execs would say today, to not revisit the concept.</p>

<p>Detroit has made poor use of new materials because they tend to cost a lot per pound.  The companies could use smarter designs that required fewer pounds, but then the door might not slam with that solid sound and what if people didn't buy.  Remember the Edsel?</p>

<p>Remember the Edsel indeed.  All the Edsel had going against it was ugly design.  In every other sense it was just another car with an engine in the front and drive wheels in the back.  The Edsel didn't fail because it was too radical, yet that's how it is remembered.</p>

<p>The leaders of the Big Three U.S. car companies have about six weeks to come up with a way to save their companies.  THEIR jobs (the CEOs) are toast, but the companies can be saved.  All it takes is a little smarts and a lot of guts to come up with faster, smarter, more efficient cars that are uniformly 50 percent lighter than the models they replace. </p>

<p>I would have suggested they consult Leeon Davis, designer of the DA-2A and many other remarkable airplanes, but Leeon died earlier this year.  He could have helped a lot, I know it.</p>

]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081126_005507.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081126_005507.html</guid>
			<category></category>
			<pubDate>Wed, 26 Nov 2008 21:37:06 -0500</pubDate>
		</item>
				<item>
			<title>Not Enough Indians</title>
			<description><![CDATA[<p>There is no joy at Yahoo, for mighty Jerry has struck out.</p>

<p>This week Yahoo cofounder Jerry Yang announced he was stepping down after 17 turbulent months as CEO of the big Internet portal -- a time in which the company rebuffed a buyout offer from Microsoft, flubbed an ad sales agreement with Google, and ended up being worth a third of its former self when the rest of the market is down only 40 percent.</p>

<p>Jerry blew it.</p>

<p>And rare in the annals of public companies, JERRY blew it, nobody else. There is no blame to be shared because the Chief Yahoo took his anti-Microsoft stand pretty much single-handed, having bounced Terry Semel from the job in June 2007. Semel, who was more Hollywood than Silicon Valley and never well suited for the job anyway, had backed the Microsoft deal. Freed from his duties at Yahoo, Semel also voted with his brokerage account, selling a large number of company shares while the selling was good.</p>

<p>If there is a lesson to be learned here it is not so much that Jerry was wrong, but that Jerry was Jerry and that wasn't the right thing for Yahoo shareholders.</p>

<p>There are three seminal ideas that guided Jerry Yang, who is, after all, a diverted graduate student who got on-the-job training in business. To understand these three ideas is to understand Yahoo under Yang:</p>

<p>1) Microsoft is evil. Yang came of age in the Netscape era and saw Microsoft break the law to destroy that company and try to control the Internet. Whatever its motivation, Microsoft did all the bad things they were accused of and more and Yang could never forget or forgive that, even at the cost of his own company. He took it personally.</p>

<p>2) The power of "no." There was a time in the 1990s when venture capitalists Kleiner Perkins and Sequoia Capital were trying to get Excite and Yahoo -- their respective portals -- to merge in a forced marriage designed to benefit only the VCs. It didn't feel right to Jerry, who put his foot down and scotched the deal. It worked that time, so saying "no" became for Yang the default position, especially after Broadcast.com.</p>

<p>3) Don't get screwed. When Yahoo bought Broadcast.com for $4.7 billion and it became clear that Yang & Co. got almost nothing of value for their money, they resolved never to get screwed on another deal again. That was the moment Yahoo embraced bureaucracy. They never made a quick decision again and in many cases hardly made any decisions at all.</p>

<p>Mix these three concepts together, add independent wealth and a personal golf course, and you get the Jerry Yang of today. He was inclined to say "no," couldn't embrace Microsoft's evil, and sure as heck wasn't going to be screwed by Redmond, which he knew could never be trusted. As long as Jerry was in command the deal would never happen -- and didn't.</p>

<p>Given all this it's a wonder Yang can remain with the company as he says he will. I couldn't do it. He must feel like Ralph Nader. Or maybe that's exactly it; Jerry Yang, like Nader, still doesn't get it.</p>

<p>The best thing Yang could have done for Yahoo shareholders was to sell the company to Microsoft. He chose, instead, to do what he thought was best for the Yahoo COMPANY, which is weird given that it no longer feels anything like it did back in those glory days. He threw away $20+ billion just to preserve a memory.</p>

<p><b>Comcast's Cap</b><br>
Hey, I have been thinking about Comcast's new 250-gigabyte monthly download cap and what to do about it. Comcast, of course, is just trying to keep its top 2-3 percent of P2P gonzos from ruining things for the rest of us. If a tiny minority of users are taking half the available bandwidth, well they have to be somehow crushed (that's the theory). Comcast first tried slowing down the miscreants, you'll remember, denied the company was doing that, then got busted by the AP of all outfits. So now they'll try this new cap.</p>

<p>As a guy who sees a GRAND PLAN nearly everywhere, of course I see one here. Comcast says the new cap will affect less than 5 percent of its users, but that's now. What happens in five years as connections get faster and faster, Internet movie and video distribution explodes and the cap doesn't rise comparably? </p>

<p>Remember wholesale bandwidth costs are dropping by 50 percent per year and have been for the last decade, so Comcast's costs get cheaper and cheaper while at the same time more and more users will impinge on the bandwidth cap. If 5 percent are in violation today, that will be 10 percent a year from now, 20 percent two years from now and 40 percent three years from now, unless Comcast raises the cap.</p>

<p>I think they won't raise the cap but will rather introduce paid bandwidth as an alternative tier and get us to start paying a la carte for those parts of our Internet experience that Comcast might presently view as under-compensated entertainment products. It's just a way for Comcast to benefit financially from third-party and user-generated content.</p>

<p>So maybe a little civil disobedience is in order. </p>

<p>That 250 gig bandwidth cap, while more than 95 percent of current users require, only comes down to 3-4 days of wide open bit-pumping on a cable modem. Why not build a utility that takes all participating users to 249 gigs per month? Even a 5 percent participation rate among Comcast users would take the network to its knees and possibly force a more respectful attitude from the cable company.</p>

<p>But hey, it's just an idea.</p>

<p><b>Cringely's Future</b><br>
Finally, readers have been asking what I'll be doing after this gig ends on December 15th. Frankly, I have no idea, but as a guy with kids ages six, four, and two, I can assure you I'm not retiring. Guys like me don't retire, we just get lots of life insurance and work until we die.</p>

<p>It might not make sense to hurl myself unemployed into the worst financial crisis in 80 years, but sometimes a guy just has to do what a guy has to do. Besides, as someone who has been fired from EVERY JOB I'VE EVER HELD, this might be my last and only chance to actually quit something.</p>

<p>I'll land somewhere, you can be sure, probably with NerdTV and my Moon shot in tow. And I'm open to ideas. Just nothing very illegal, okay?</p>
]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081118_005506.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081118_005506.html</guid>
			<category></category>
			<pubDate>Tue, 18 Nov 2008 18:18:48 -0500</pubDate>
		</item>
				<item>
			<title>Now For Something Completely Different</title>
			<description><![CDATA[<p>President-Elect Barack Obama has announced that when he's in office he'll appoint a Chief Technology Officer (CTO) for the whole darned USA.  Though Google CEO Eric Schmidt already said he isn't interested in the job, I am. </p>

<p>I accept, Mr. President. </p>

<p>And while the idea of Cringely for CTO may seem lame to most everybody I know (including my Mom), I think I can make a strong case for why I am EXACTLY the right guy for the job. </p>

<p>For one thing, unlike Eric Schmidt I don't have a lot of money.  Schmidt can't afford to take the job because Google stock is down and he'd lose a fortune.  Not so for me.  I come encumbered only with debts, which is to say I am a true American.  I'd be perfectly willing to put those debts in a blind trust ASAP. </p>

<p>The U.S. CTO would have to be a dynamic leader capable of speaking his or her mind and holding his or her own against a tide of critics and special interests.  Hey, that's what I do every week (sometimes twice)!  Maintaining and defending technology opinions is my only business and some people think I do it too well, which I take as a compliment. </p>

<p>Now we need to consider why President-Elect Obama thinks the country needs a CTO in the first place.  The President has long had a Science Adviser, so why appoint a CTO?  It's the distinction between adviser and officer that I'd say is the whole point; one simply advises while the other implements and leads directly.  And I think there is plenty of room for new leadership in this area. </p>

<p>America has always been tops in science, tops in research and development, tops in medicine, tops in industrial development, tops in technical infrastructure -- tops, tops, tops.  But are we tops today?  I don't think so, and I'd say we've been slipping steadily for the last eight years and probably for many years before that.  The rest of the world has caught up and some other countries now lead the U.S. in many respects.  Yes, we have technical traditions and deep institutions, but those traditions are weaker than they were and the institutions are, too.  I think something can be done about that. </p>

<p>My belief that something CAN be done is critical, because most of the usual suspects for this job probably think it can't.  The reason I am so optimistic is because of the very financial disaster that is the current U.S. economy.  Things are so bad right now that I am greatly encouraged. </p>

<p>Huh? </p>

<p>Sometime in February the new Obama Administration is likely to propose the mother of all economic stimulus packages.  It won't be a $650 check that comes in the mail.  It won't be a $700 billion equity injection in various financial institutions.  It WILL be a public spending plan modeled after the New Deal of the 1930s, injecting $600+ billion primarily into infrastructure construction and reconstruction.  The difference between this New New Deal and the first one is that while plenty of roads and bridges will be rebuilt, a lot of the money this time will probably go into information infrastructure.  Well that's my bag. </p>

<p>The U.S. CTO - at least this FIRST U.S. CTO - will be the buyer-of-cool-stuff-in-chief for the entire nation. </p>

<p>I would make a better buyer-in-chief than almost anyone else because of two important characteristics in my warped personality: 1) I would be immune to special interest groups so this wouldn't turn into another National Information Infrastructure boondoggle, and; 2) yet as a true enthusiast I would buy with such reckless abandon that I'd easily fulfill the economic stimulus needs while spewing money widely enough to guarantee at least a few good technical investments for the nation. </p>

<p>This latter point probably requires some explanation.  As we can see from the current $700 billion bank bailout, the ranks of those actually benefitting are pretty small.  We're $325 billion into the thing and consumers - the people paying for it -- have yet to benefit at all as far as I can tell.  Most banks haven't even benefitted.  And those that have benefitted have done little to share their wealth.  To put things in the most positive light I can, let's attribute this to the very surgical nature of this process.  To put it more honestly, nothing really changes except the rich get richer. </p>

<p>Look at Al Gore's National Information Infrastructure program of the 1990s, which was intended to build for us all exactly the sort of data network enjoyed today by people in Japan and Korea.  $200 billion in tax credits were distributed, primarily to telephone companies.  That's $200 billion in government revenue foregone, which is just the same, it seems to me, as writing a check.  And what did we get for it?  Limited Internet service in schools and no Internet service in homes.  The DSL we have today we paid for, believe me - phone companies sell that stuff at a profit.  However well intentioned Al was, his system was gamed by the phone companies who took the money and ran. </p>

<p>That can't happen again. </p>  

<p>If we are going to have a huge economic stimulus package that we'll pay-off as a people over the next 30+ years, I say we should get something for it.  If we hire as CTO some slick-talker from IBM or GE this won't happen.  If we hire Bill Joy it won't happen, either.  Bill's too smart and too gentle and too darned rich for the job. </p>

<p>We need someone with just enough savvy to know good technology, enough independence to make the right decisions, and crazy enough to do it all 24/7 right out in public so that vaunted "transparency" we keep talking about yet never see can be proved to be more than just a modern myth. </p>

<p>I'm the man for that job. </p>

<p>AND I can use the work. </p>

<p>That's because December 15th will mark my last column for PBS. </p>

<p>After 11 years and more than 600 columns I'll be moving-on, perhaps into that big CTO job in Washington, but then maybe not.  This is my decision, not that of PBS, which has been nothing but good to me these many years. </p>  

<p>In the month I have left I will be filing many columns, trying in a breathless rush to put a cap on this part of my career and leave behind a few ideas of how things should be and where they can go if done right.  Though it will be a couple weeks early, the last column will be my predictions for 2009.</p>

<p>Stick around until then.  I'm right most of the time, you know. </p>   

]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081114_005505.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081114_005505.html</guid>
			<category>Technology</category>
			<pubDate>Fri, 14 Nov 2008 15:57:16 -0500</pubDate>
		</item>
				<item>
			<title>Love-Hate</title>
			<description><![CDATA[<P>Steve Jobs is not like you and me.  He has millions of customers, 32,000 employees, and a board of directors who think he can do no wrong.  Running a company that is immensely profitable, gaining in market share, has no debt and $20 billion in cash, he can afford to make bold moves, the most recent of which is his decision to replace Tony Fadell, until moments ago head of the division that produces Apple’s iPod.  Like everything Jobsian, Fadell’s departure is part of an Apple GRAND PLAN.<P>
<P>The variables at work here are (in no particular order) ego, competitive advantage, ego, management technique, ego, strategic thinking, and ego.<P>
<P>To say that Steve Jobs’ ego can expand to fill any known space might be an understatement but I’ll stand by it anyway.  Fadell’s failing in this regard is his being hailed as the “father of the iPod.”  What does that make Jobs?  Who made THE BIG DECISION?  Who committed the company?  Who – most importantly of all – seduced all the record companies?  That last guy would be James Higa, but since I don’t want to get HIM fired, too, let’s just attribute it all to Steve Jobs – for all intents and purposes the REAL father of the iPod.<P>
<P>All hail Steve.<P>
<P>Apple exists solely as an extension of Steve Jobs.  Remember that.  Anything attributable to Apple is really attributable to Jobs.  Other people work at Apple, of course, and excel at their positions, but that is primarily because they were chosen, anointed, or inspired by Jobs.<P>
<P>Not that Jobs doesn’t make the occasional mistake.  Look at the Mac Cube, for example.  But that was our mistake as consumers, not realizing that it really ought to have been worth an extra $500 to us to have a computer with no cooling fan.<P>
<P>Steve Jobs makes very few such mistakes, in fact.  That, and his total domination of Apple at every level allow the company to be literally the only PC vendor to have anything like a strategic plan.  Dell and HP have the odd strategic initiative, like getting into or out of media players or TVs, but the idea of a comprehensive corporate strategy, well that’s too much to expect from companies that are managed, not led.<P>
<P>Steve Jobs is a leader 100 percent in the mold of General George S. Patton.  Rent the movie and it will start to make sense.  Heck, rent it on iTunes.<P>
<P>So here’s what’s going on with Tony Fadell.  First, he was vulnerable as a charismatic leader in his own right who has been talked about in the press as a possible heir to Jobs.  That alone meant he had to die, but it wasn’t enough to mean that he had to die just now.  That decision required an external variable in the form of former IBM executive Mark Papermaster.<P>  
<P>Steve Jobs wants to give Tony Fadell’s job to Papermaster.  It’s not that Papermaster would be any better at the job than Fadell, but there are two over-riding factors here: 1) Jobs can only have so many direct reports, and; 2) he thinks putting Papermaster in Fadell’s job is the best way to get past any legal objections from Papermaster’s former employer, IBM.<P>
<P>Papermaster most recently ran IBM’s blade server division and in the mind of Steve Jobs blade servers and iPods couldn’t be farther apart.  One is an enterprise sale while the other is consumer.  One is a clear IT sale and the other has nothing to do with IT, really, since iPods and iPhones aren’t aren’t computers or computer peripherals.  Jobs thinks Apple can make this point stick with a judge and he might well be correct.<P>
<P>Papermaster has to be gone from IBM for a year before he can take a job that clearly competes with his last position at IBM.  But Jobs doesn’t want Papermaster for blade servers, nor does he even want him for iPods.  Jobs wants Papermaster for the expertise he showed two jobs ago at IBM running Big Blue’s PowerPC operation.  Jobs wants Papermaster to lead Apple’s PA Semi acquisition and create a new family of scalable processors optimized for Snow Leopard and beyond.<P>
<P>Having Papermaster run iPod hardware is a placeholder to let him get used to Apple and get ready to take over the Apple processor job, some of which will be used in iPods and iPhones, so the job isn’t a total waste.  But for the few months he’ll be running iPod hardware, Papermaster will mainly be overseeing the implementation of Fadell’s strategy.<P>
<P>If that seems like a game of musical chairs in the Cupertino executive suite, well it is.  It’s also a game we’ve seen played over and over again.<P>
<P>Back to point 1 from five paragraphs ago: Jobs can have only so many direct reports.  Steve Jobs believes the key to his success is in finding, hiring, retaining, then firing the best talent in the world.  He would maintain in the very moment he’s firing Fadell that Tony is better at his job than anyone else on Earth.  Yet still Fadell must go and that’s because – ego issues aside – Jobs had to make room in his inner circle for Papermaster.<P>
<P>Everyone close to Jobs is under continual analysis: is this person really (or still) the best in the world?  If they aren’t, or if someone else is just as good but more important for some additional reason, then the incumbent has to go.  Steve Jobs ultimately betrays all of his direct reports in this manner.  It’s just the way he is.  And if it costs Apple a few million to remove one extra head from the room, well that’s okay with a board that KNOWS (as we all do, to put it fairly) that Jobs really is the secret of Apple’s success.  His system may be brutal, but it works.<P>
<P>So Fadell was already in danger because he had become known as an individual.  Remember that when PortalPlayer (now part of nVidia) was making the guts of every iPod the company was forbidden by Apple to acknowledge that.  Even in its financial reports PortalPlayer was forbidden to use the “A” word and simply had to attribute to some unnamed company 85 percent of PortalPlayer’s revenue.<P>
<P>Just as Jobs was scourging Fadell, though, he was seducing Papermaster.  Jobs can be VERY seductive.  And he was hardly going to seduce the IBM executive with a promise to put him two levels down.  So as the most vulnerable person in Jobs’ inner circle, Fadell had to go.  That Fadell’s wife was head of Human Resources for Apple and was forced, essentially, to terminate her own husband, well that was just gravy and yet another reason for Apple employees to take the stairs rather than risk sharing an elevator with Jobs.<P>
<P>Fear can be a remarkable motivator.<P>
<P>Don’t feel bad for Fadell, though.  His $8+ million golden parachute stock grant is coming at a time when Apple shares are depressed and could easily double by the time he can sell them in 2010.  He get’s $300,000  per year to “advise” Jobs (I’d like one of those jobs, too, Steve) and then there’s his wife’s departure package, which hasn’t been mentioned.  Clearly out of the picture as an heir to Jobs, Fadell will next appear in 2010 as a CEO somewhere in the South Bay.<P>
<P>Of course IBM with its largest corporate legal department on earth has filed suit against Apple, trying to block Papermaster from taking the Apple position.  Apple’s legal department is fairly accomplished, too, and Cupertino is a much stronger company than Armonk, which will lead to the ultimate solution to this legal problem.  Apple still hopes to convince a judge that it is correct about Papermaster.  But if Apple fails in that, Steve Jobs will just pick up the phone and choose IBM Microelectronics as the fab to build the next generation of Apple’s PowerPC processors – a contract worth billions, but ONLY if IBM drops all legal action.<P>
<P>Apple will win in the end -- I guarantee it.  And the way Jobs negotiates, Big Blue will probably end up losing money on the chip deal, too.</P>
]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081107_005504.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081107_005504.html</guid>
			<category>Technology</category>
			<pubDate>Fri, 07 Nov 2008 14:48:12 -0500</pubDate>
		</item>
				<item>
			<title>Azure Blues</title>
			<description><![CDATA[<p>It isn't very often I get to apply Moore's Law to a non-Information Technology business and rarer still that I can then relate the whole thing back to Microsoft, so I'm going for it. Here's what the solar power industry can teach us about Microsoft:</p>

<p>The wonderful thing about Moore's Law is what the lady at the bank called the "miracle of compound interest." That halving of manufacturing cost every 18 months (the OTHER way of looking at Moore's Law that we generally don't use) has little apparent impact in the first few years, but eventually the halving and re-halving takes a real bite out of the cost side until substantial performance is very, very cheap. That explains why there is more computing power -- a LOT more -- in your iPod than was required for the Apollo Moon missions. Well this applies to ALL silicon-substrate photolithography applications, not just computer chips. It applies equally well, for example, to silicon solar cells.</p>

<p>There are many types of solar cells. Some solar cells involve crystalline silicon just like computer chips and others use amorphous silicon, but all types benefit from Moore's Law. In fact one especially good aspect of solar cells is that they can make use of older process technologies that are obsolete for computer work. So every time Intel or AMD builds a new fab there is a market in the solar industry for their old machines. Look at those round solar cells used in many arrays today and you'll notice the smaller wafer sizes favored in Silicon Valley 15-20 years ago. That's no coincidence. </p>

<p>The result of this relentless application of Moore's Law to the solar industry is that we can see a time in that near future when the cost of producing a watt of electricity from a solar cell on your roof will be approximately the same as the cost of delivering that same watt over a power line from an electric utility. And of course that means that 18 months after that point the solar watt will cost HALF of what the same power would cost from the electric company, which will completely change the game.</p>

<p>The time when that electricity cost parity will be reached, I'm told, is seven years from now. Just think of the impact that will have on electric utilities! Why would any of us continue to buy our power from them? We might use them as a giant storage battery and possibly for backup on cloudy days, but why would we use them at all for power if we can generate it cheaper at home? You can bet that's a question the electric power generating industry is asking itself.</p>

<p>The whammy for the power companies is two-fold, because not only will power be cheaper but, by definition, the cost of building and installing solar panels will be substantially cheaper, too, than it is today. If it costs $40,000 on average to refit your house today, a lot of homeowners can't afford that, but what if it becomes $10,000? That's what worries electric companies that are used to having easier access to capital than do their customers. But once installing solar power costs relative chump change (the cost of a nice Ski-Doo or remodeling a bathroom), we'll see massive conversion and the power companies know that.</p>

<p>So what can they do? They can find ways to get us to use more power than can possibly be generated from the roof of a typical American home. And that's why this week the Electric Power Research Institute proposed that we all get plug-in hybrid cars. It would save billions of barrels of oil, they say, lower greenhouse gas emissions, clean the air, oh and by the way require more electricity than your solar cells can produce, thanks.</p>

<p>And it will work -- for a while. But Moore's Law is relentless, you know, and the role of electric utilities will change dramatically over the next decade as a result. As far as I can see, this is all for the better.</p>

<p>But what does it have to do with Microsoft?</p>

<p>Well that brings us to Windows Azure, which was still called Windows Cloud when I first mentioned it a couple weeks ago. Like all Microsoft strategies, Windows Azure is a reaction to external competitive pressures. And it is important, VERY important. Here's how a source of mine at Microsoft put it a few days ago, before the Azure announcement: "The cloud stuff isn't just another enterprise product. It is going to impact everything we do -- all of the product groups -- consumer and enterprise -- are going to have to figure out where they fit in to the cloud paradigm. The shift to cloud-based computing is analogous to our shift to the Internet in the late '90s. It changed the direction of the company and impacted everything we did."</p>

<p>Wow, that's a big deal! Yet based on the Microsoft announcement this week, all Windows Azure looks like to me is Microsoft's effort to sell web services or maybe cut the sticker shock for smaller businesses adopting SQL Server. But more properly, it likely means Microsoft's acceptance that computing clients may eventually be free or nearly so. In short, Windows Azure is an insurance policy against the possible Vista-like failure of Windows 7.</p>

<p>Last week, for example, I wrote about Microsoft's Windows Mobile technology, predicting that it would die simply because Redmond would realize that it could never be first or second in market share. That was no big scoop from me, though some news people took it as one -- it was just common sense. And so what happened this week? Well here's a report from a reader attending Microsoft's Professional Developer Conference, where Windows Azure and Windows 7 were introduced this week.</p>

<p>"Windows Mobile has (a) near zero presence at MS PDC," wrote the reader. "Their Live Mesh platform has Windows Mobile as an integral component but otherwise no mention, no sessions. There was one session scheduled but it was cancelled at the last minute. Hmmm."</p>

<p>When the body is under stress, it eventually sacrifices entire limbs to keep the internal organs working. Windows Mobile is just an appendage to Microsoft and always has been. Yet mobile is clearly the client of the future, so what's to be done? Windows Azure. Control the back end through industry standard -- even open source -- protocols. Make money from subscriptions and ads -- make money any and every way in the hope of leveraging a global infrastructure investment into a continuing business strategy.</p>

<p>Can you see the connection here? There is almost no difference between Microsoft trying to become our computing utility and the electric company trying to power our next-generation cars. Both are coping strategies, both are risky, but neither Microsoft nor the electric utilities see that they have any real choice. And maybe they don't.</p>

<p>For Microsoft, at least, it could be a strategy with legs. While the utilities will be undercut more and more by Moore's Law, Microsoft as a computing utility won't be. But that doesn't mean they'll be any good at the job. It means fighting a war on two fronts -- with Google as a provider of applications and with Apple as a provider of content. MAYBE Microsoft has a shot against Google, which is becoming more Microsoft-like itself by the day, but to compete with Apple as a content provider? Forget it. Microsoft simply isn't the class act it needs to be to dominate that space, so look for acquisitions to (maybe) fill that void.</p>

<p>And all this means that Windows 7 had darned well better hit a home run or Microsoft is in BIG trouble.</p>

]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081030_005501.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081030_005501.html</guid>
			<category></category>
			<pubDate>Thu, 30 Oct 2008 14:21:28 -0500</pubDate>
		</item>
				<item>
			<title>Collateral Damage</title>
			<description><![CDATA[<p>I am not a very sophisticated mobile phone user.  I don't use most of the bells and whistles on my phone, probably because I don't know what they even are.  But just because I'm an idiot about USING mobile phones doesn't mean I don't understand the emerging mobile market, to which I have been paying a lot of attention of late.  And why not?  As personal computers fade from what Al Mandel called "ubiquity to invisibility," something has to take over.  And everyone I respect thinks the new dominant platform will be mobile.  So it's my job to tell you, then, that Windows Mobile is probably doomed.</p>

<p>Interestingly, this conclusion isn't based on any personal preference or subjective analysis.  I'm not saying that Windows Mobile is bad, just that it is probably doomed.  It's a simple matter of market economics.</p>

<p>There is generally room in any technology marketplace for three competing standards.  Notice I say "standards," not "brands."  There can be many brands of road vehicles, but they generally come down to cars, trucks, and motorcycles -- each a standard.  In personal computers we have Windows, Macintosh, and Linux (or similar Unix workstation variant).  In HVAC systems, just to stretch the point, there are radiant, forced air, or evaporative systems -- again three standards.</p>

<p>And among those three standards there tends to be a market-share distribution that is more or less 85-10-5.  These numbers can jump around a bit and one can argue that the Mac is now more than 10 percent of recent sales, though not of the installed PC base, so I hope you get my point.  This magic 85-10-5 distribution also happens to mirror what happens at the racetrack or in the casino, where 85 percent of gamblers lose, 10 percent break-even, and 5 percent are winners, which explains all those big buildings in Las Vegas.</p>

<p>The mobile phone marketplace shows a similar distribution, though that now appears to be in some transition.  One could argue that the old 85-10-5 came down to basic or dumb phones (85), smartphones (10), and specialized or vertical phones like the old Nextel (5).  Moore's Law now seems to be inexorably turning all phones into smartphones, so we're probably moving toward an 85-10-5 based on programming platform.</p>

<p>Let's consider this smartphone migration for a moment, first with Samsung in mind.  Last week Samsung announced that it would no longer be making high-end phones and would stick to basic phones in the future -- going for higher volumes at lower cost.  This makes a lot of sense given that sophisticated phones must cost more to develop yet tend to be more expensive as a result and therefore have lower sales numbers.  So why bother?  This was the message Samsung sent out and everyone bought, but I really think that if you look at it in the context of a dynamic market the announcement means something else altogether.</p>

<p>Deconstructing the Samsung announcement we'd have to wonder how the company sees itself and its competitors.  That answer is pretty simple: Samsung sees itself as a global electronics company competing with outfits like Sony.  Samsung has been for 40 years all about copying and eventually crushing Sony.  Now given that's how Samsung sees itself (nobody I know contests this vision, by the way), how can the company possibly afford to let Nokia, Motorola, Sony, and Apple make high-end phones, which is to say smartphones, without Samsung competing in that space?  That would be giving up a lifelong dream and Samsung just won't do it.</p>

<p>So were they lying?</p>

<p>No, Samsung wasn't lying, they were just doing what my old friend, PR man Martin Quigley, called "dissembling."  Samsung probably has no intention of abandoning the smartphone market because ALL phones are becoming smartphones.  What they truly intend to do, however, is make smartphones that are generally inexpensive, hoping to gain market share as a result.  We'll see this trend from Apple, too, which will push iPhone prices down and introduce cheaper models next year and beyond.</p>

<p>While there are many ways for Samsung to make smartphones less expensive, the easiest way to do so and yet remain competitive on features is by no longer using software that costs money.</p>

<p>In the smartphone space there are, at present, only three operating systems that are being broadly licensed on an OEM basis -- Android, Symbian, and Windows Mobile.  Of those three, two are free -- Android and Symbian.  Symbian didn't used to be free but times change and now it is.</p>

<p>So Samsung was announcing that it would be ending development of Windows Mobile devices at some point, though they never said that directly.</p>

<p>Sticking with Samsung for a moment, then, which of the two free software platforms is the company likely to endorse?  That's a good question.  Symbian has a very strong presence in Japan, which is an important market for Samsung, so I don't see them abandoning Symbian immediately.  But in the longer term I think Samsung WILL abandon Symbian, as will most of the rest of the world.</p>

<p>Here's why (donning flameproof clothing): Symbian is simply too old. The OS is getting slower and slower with each release. The GUIs are getting uglier and are not user-friendly. The development environment is particularly bad, which wouldn't hurt if there weren't others that are so much better.  Symbian C++, for example, is not a standard C++.  There is little momentum in the Symbian developer community, maybe because coding for Symbian is a pain. Yes, there are way more Symbian phones in circulation, but those phones will be gone 18 months from now, probably replaced by phones with a different OS.  Lately, Symbian's success has been primarily based on the high quality of Nokia hardware, on the loyalty of NTT DoCoMo, and now on the lure of being recently made open source and therefore free.  But if open source developers don't flock now to Symbian (they aren't as far as I can see -- at least not yet) then the OS is doomed.</p>

<p>My guess is that in time Samsung, like Motorola, will devote its smartphone development 100 percent to Android.</p>

<p>Maybe, but what about Apple and RIM, what will happen there?  This is not a time to bet against the iPhone, which is changing the entire landscape of not just smartphones but mobile phones in general.  For all its teething problems, there is a new sheriff in town and his name is iPhone.  We'll see nothing but progress and market-share gains there for at least another two product cycles or three years.</p>

<p>RIM is another story altogether.  What RIM has going for it are loyal users, good keyboards, and push mail.  Most mobile phone users still think RIM is the only platform that has push mail.  But given that push mail will soon be everywhere and the market will eventually figure that out, RIM is facing a huge challenge.  I'm not saying they won't meet that challenge, I simply don't know.</p>

<p>If I had to bet right this moment on the mobile 85-10-5 of 2011 I'd say iPhone, Android, then RIM, Symbian, or something completely new from behind Door Number Three.</p>

<p>Why iPhone over Android?  For exactly the same reason why the iPod holds that approximate 85 position among music players, including ones using open source software. iPhone has a really great SDK (light-years ahead of any other right now).  The App Store distribution platform is great, but locked on too many points. This is a careful timing issue for Apple.  If they open the APIs too quickly they risk being blocked. They need to open an API once they are perfectly sure it is the right one and the right way to export that function. Apple is going to relax the restrictions progressively when they better understand the use cases and what are the best APIs. In the meantime it is giving an advantage to Android, but one that I think a year from now Apple will have reclaimed. </p>

<p>And where will Windows Mobile be in 2011?  There way things are headed now, given that Microsoft can't really afford to be anything but first or second on the platform that supplants Windows, I'd say Windows Mobile will be dead.</p>
]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081023_005500.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081023_005500.html</guid>
			<category></category>
			<pubDate>Thu, 23 Oct 2008 13:23:10 -0500</pubDate>
		</item>
				<item>
			<title>Ctrl-Alt-Del</title>
			<description><![CDATA[<p>Apple last week introduced a pair of very nice notebook computers that, not at all surprisingly, looked like riffs on the MacBook Air.  The company in a separate announcement released 600 high-definition television episodes through the iTunes Store. This week Apple will reportedly release new 20-inch and 24-inch iMacs, also for the Christmas season. Two weeks, three announcements, but what strikes me (and apparently only me at this point) is what won't be announced -- the big surprises that are missing.  What happened?</p>

<p>A MacBook and a MacBook Pro are nice, but not overwhelming.  I like the dual GPU in the Pro and I hate the lack of a FireWire port in the MacBook, but beyond that there is little to say about these products except that the glass screens (on the iMacs, too) are better for houses like mine filled with LCD screen-destroying pre-school boys.  These new products don't appear to break any price or performance barriers and sure as heck don't allow time travel or make me more handsome. </p>

<p>We were led to expect more -- a lot more.  And I am not talking about rumors.</p>

<p>Back on July 21st in his regular conference call with industry analysts, Apple Chief Financial Officer Peter Oppenheimer said that Apple's profit margin would likely shrink from 34.8 percent in the just-concluded quarter to 31.5 percent in the quarter ending in September. "We've got a future product transition that I can't discuss with you today," Oppenheimer said as he spelled out the reasons for the anticipated profit reduction. "One of the reasons that we see gross margin being down sequentially is because of a product transition."</p>

<p>What kind of Apple product could be expected to come along, taking a $244 million profit hit for the company?  It certainly isn't any of the products we've discussed so far, nor is it the iPhone 3G or the new iPod Touch, which have both been publicly dissected and found to have gross margins in the 56 percent range.</p>

<p>It's something else that was probably intended to be announced this week but wasn't.</p>

<p>The change of plan could have come for many reasons.  Maybe the revolutionary product wasn't ready in time.  Maybe introducing an aggressive, low-margin product in the middle of a global financial crisis was considered a bad risk.  Maybe some strategic alliance had to be in place and wasn't ready.  Whatever it was, the same analysts will ask about it this Tuesday when Apple has another such conference call scheduled.</p>

<p>But of course none of this keeps me from speculating about what's missing from Apple's announcements and the reasons it might be missing.</p>

<p>I think the delayed product has everything to do with Apple's desire for Blu-ray DVDs to die as a standard.  Apple CEO Steve Jobs took a swipe at Blu-ray in last week's announcement -- a swipe that felt out of sync with the rest of the program.  Steve has no difficulty at all NOT talking about subjects he wants to avoid, so leaning into Blu-ray was not at all offhand or without strategic importance.  Don't expect Blu-ray drives on Apple computers, Steve said, yet he didn't offer a clear alternative.</p>

<p>The alternative Jobs would like to offer, of course, is full 1080p HD video distribution on iTunes, but that's not currently possible.  It will happen in time, of course, but certain prerequisites have to be in place.  Apple hardware has to support it in a practical sense, for one.</p>

<p>Interestingly, users of the new Apple notebooks began reporting that CPU utilization for H.264 decoding on their new machines dropped from 100 percent on an earlier model with the same processor to sub-20 percent on the new aluminum MacBooks.  Though it wasn't announced, Apple seems to have (finally) enabled H.264 decoding on the Nvidia GPUs in these new machines.</p>

<p>Equally significant is the fact that ONLY H.264 appears to be accelerated.  HD content using the MPEG-2 or VC-1 codecs seem to be not accelerated, which means this improvement is aimed specifically at Apple (iTunes) content, NOT physical media.</p>

<p>This is something we might have expected Apple to trumpet, but they don't offer any 1080p content yet other than movie trailers.  Maybe it is better, Apple might imagine, to pre-seed this capability so more machines can take advantage of it when Apple 1080p content finally does appear.</p>

<p>What's yet to come, I'm guessing, is Apple's next OS X release, Snow Leopard, with QuickTime X -- the first version of QuickTime supposedly optimized for H.264 hardware decoding.</p>

<p>Snow Leopard is late, but then operating system updates are always late, no matter the vendor.  This delay could be for any number of reasons and there are probably several, but one of them I can guarantee you has to do with H.264.</p>

<p>More than a year ago I made a big point of predicting that Apple would go to H.264 hardware acceleration, though I pinned it on a specific chip from NHK and NTT in Japan.  This was after the usual evening of drinking with Japanese executives that typically reveals such information.  Oh the sacrifices my liver makes for journalistic integrity!</p>

<p>So what happened to that NTT chip?  I don't know.  Maybe it was too expensive and fell out of the plan.  Maybe it's in there still and Nvidia licensed technology from NHK and NTT to enable the new hardware acceleration (this is just a speculation -- I'm not at all saying they did).  Maybe -- and this is the one I believe -- the discrete NTT chip was overtaken by a snowballing Apple strategy involving PA Semi, Apple's recently acquired division that designs microprocessors.</p>

<p>Here's the reasoning, which isn't all mine by any means.  I have the smartest readers in the world and they are constantly giving me new things to think about.</p>

<p>First we see Apple moving away from Intel chipsets with these new MacBooks and probably with the iMacs to be introduced this week.  Apple has always been involved in its own chipset design and giving up that capability to Intel until now didn't make much sense, especially considering the crappy Intel integrated graphics.  It is logical to assume that Apple would reclaim its right to design or at least specify the chipset as soon as its internal engineering capability could support that.</p>

<p>Turning to Nvidia isn't the same as doing the chipset itself, but you can bet Apple's fingers are all over the chipsets in these new machines and that they are significantly different from those in notebooks from companies like Dell and HP.</p>

<p>There is a reason to be different here that goes beyond performance.  That other reason is Psystar, the would-be seller of Mac clones with which Apple is now locked in a legal battle that could go either way.  What if Psystar comes out on top and has the right to sell Mac clones based on the Hackintosh model?  Then Apple will have to break that model by becoming more proprietary and therefore harder to emulate.  Enter the third-party chipset, which is just the first step in Apple's effort to become immune to Psystar-type clones no matter what the court decides.</p>

<p>Second is Snow Leopard, itself.  Apple has suggested that Snow Leopard won't have many new features but will be Apple's effort to more fully take advantage of multi-core processors.  This harkens back to my parallel computing column of a couple weeks ago.</p>

<p>Snow Leopard, I'm told, will make seamless use of as many cores as are available.  It isn't clear whether applications will have to be rewritten to take advantage of this capability, but I'm guessing they will have to be.  This is just a guess, mind you, but is consistent with the sort of demands Apple likes to place on developers.  Apple's own applications will be Snow Leopard-compatible you can bet, and will set a daunting performance standard in iMovie and Final Cut Pro.</p>

<p>Where PA Semi fits in is by providing to Apple a modular, Intel-compatible multi-core architecture that can scale to cover entire future Apple product lines.  By dishing out more responsibility to the GPU, Apple can enable a much simpler CPU with as many cores as needed.  Imagine a single core in an iPhone, two cores in an Apple TV, 2-4 cores in a notebook, 4-6 in an iMac, and 8+ in a Mac Pro.  Wait a year then refresh all those platforms by doubling the number of cores with no change in software.</p>

<p>Moving to its own microprocessors would maintain Windows compatibility (though possibly at some lower performance level), cut hardware costs by $200 or so, and make it that much harder for others to build Mac clones in the future.</p>

<p>And what about the jump to 1080p video distribution on iTunes?  That will require faster hardware, especially on the Apple TV, which really needs a refresh.  It would have been nice to introduce all of this for Christmas, but I'm not surprised it slid.  And maybe January MacWorld is better, anyway, if Apple can also introduce new Mac Pros for content creation and those rumored giant Apple displays (HDTVs) with their built-in Apple TVs.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081020_005499.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081020_005499.html</guid>
			<category></category>
			<pubDate>Mon, 20 Oct 2008 15:21:49 -0500</pubDate>
		</item>
				<item>
			<title>Cool Threads</title>
			<description><![CDATA[<p>A couple of columns ago we touched on the practical rebirth of parallel computing.  In case you missed that column (it's in this week's links), the short version is that Moore's Law is letting us down a bit when it comes to the traditional way of increasing the power of microprocessors, which is by raising clock speeds.  We've hiked them to the point where processors are so small and running so hot that they are in danger of literally melting.  Forget about higher clock speeds then; instead we'll just pack two or four or 1000 processor cores into the same can, running them in parallel at slower speeds.  Instantly we can jump back onto the Moore's Law performance curve, except our software generally doesn't take advantage of this because most programs were written for single cores.  So we looked back at the lessons of parallel supercomputers, circa 1985, and how some of today's software applies those lessons, such as avoiding dependencies and race conditions.</p>

<p>But we didn't really talk much in that column about the use of threads, which are individual processes spun off by the main CPU.  Each time the microprocessor adds a new task, it creates a thread for that task.  If the threads are running on the same processor they are multiplexed using time-slicing and only appear to run in parallel.  But if the threads are assigned to different processors or different cores they can run truly in parallel, which can potentially get a lot of work done in a short amount of time.</p>

<p>Most traditional PC applications are single-threaded, meaning the only way to make them go faster without a completely new architecture is to run the CPU at a faster clock rate.  Single-threaded apps are simpler in that they are immune to the dependencies and race conditions that can plague true parallel code.  But they are also totally dependent on tasks being completed in a quite specific order, so in that sense they can be dramatically slower or dramatically more resource-intensive than multi-threaded apps.</p>

<p>For an example of where single-threaded applications are just plain slower, consider Eudora, which is still my favorite e-mail client (I'm old, you don't have to tell me).  Until not very long ago Eudora still couldn't send mail in background, so everything (and everyone, including the user -- me) had to wait until the mail was sent before completing anything else, like writing a new message or replying to an old one.  I KNOW THIS IS NO LONGER THE CASE, SMARTY-PANTS -- THIS IS JUST AN EXAMPLE.  The program was single-threaded and, since sending mail is a very slow activity, users were generally aware that they were waiting.  Today Eudora sends mail in background, which is the same as saying "in another thread."</p>

<p>Multithreading has been great for user interactivity because nothing should ever stop the input of data from typing, mouse movements, etc.</p>

<p>There are many ways to use threads and before we consider some let's think about scale -- literally how many threads are we talking about?  To run at true clock speed we'd have only one thread per CPU core, but a fast processor can multiplex hundreds or even thousands of threads and multi-core processors can do even more.  So the EFFICIENT shift to multi-threaded programming requires a significant change in thinking on the part of developers.</p>

<p>Here's an example: A hard problem with programming games is when you want something to happen every so often. That's not very efficient to code because it traditionally requires a program loop that spins as fast as the CPU will let it (making the CPU go to 100 percent) and keeps checking the time to see if it is time to do that thing.  But threads are different: With threads you can very easily put them to sleep for any period of time, or even put them to sleep indefinitely until some event occurs. It's not only easier for the programmer, it takes nearly no CPU power compared to the looping system.</p>

<p>How do you use threads to write an e-mail server that handles thousands of simultaneous incoming e-mails?  Well, you write it as if you were writing a server that can only handle ONE e-mail at a time.  Just write very simple code that knows how to accept e-mail, then test it by sending in an e-mail.  It works?  Cool.  Now send 1000 threads into that same piece of code. Each thread has its own state as to what e-mail (FROM, TO, SUBJECT, etc.) it is receiving, but despite the fact that that the content is different, the process is exactly the same.  Now you have an e-mail server capable of serving a thousand simultaneous connections.</p>

<p>See, writing multi-threaded apps may require a different approach but the benefits from doing so can be fantastic.</p>

<p>A new area of multi-threaded programming that is REALLY hard (hard even for developers who do normal multi-threaded programming really well) is the use of optimistic concurrency.  Two columns ago I alluded to this in my example of Appistry's decision to forego using a database for a credit card processing application.  I said I would show a hack that was yet another approach to the same problem.  Well here comes the hack.</p>

<p>Let's consider this problem: My wife (the young and lovely Mary Alyce) and I happen to be in different parts of town, each of us standing in front of a bank ATM machine.  I am a thread, Mary Alyce is a thread, and the ATM is main memory.  Contrary to our usual behavior in which we only take money OUT of the bank, we are paradoxically planning near-simultaneous bank deposits. Our balance starts at $1000. I am depositing $200 while Mary Alyce is depositing $300. </p>

<p>The ATM does the process like this:</p>

<ol><li>Retrieve existing balance</li>
<li>Add new deposit to that balance</li>
<li>Update new total balance</li></ol>

<p>The trick is that we are both doing this same thing at the same time. What if this happens:</p>

<blockquote>Bob starts deposit transaction<br>
Mary Alyce starts deposit transaction<br>
Bob's ATM grabs the balance ($1000)<br>
Mary Alyce's ATM grabs the balance ($1000)<br>
Bob deposits $200 (his ATM updates the balance to $1200)<br>
Mary Alyce deposits $300 (her ATM updates the balance to $1300)<br>
Bob's ATM updates the main database with the new balance ($1200)<br>
Mary Alyce's ATM updates the main database with the new balance ($1300)<br></blockquote>

<p>This is a race condition. Mary Alyce updated the balance last so my deposit is lost completely.  There are many ways for this to work out still, but also many ways for it not to work out OK.  And any traditional solution requires a LOT of back-end reconciliation and computation.  We need something simpler.</p>

<p>Classic concurrency control would basically LOCK the database.  This is called, not surprisingly, "pessimistic concurrency." So I go up to the ATM and my ATM requests the database to be locked for me. If Mary Alyce then went to another ATM it would tell her she has to wait because the database was being updated elsewhere.  This ensures that the race condition can't happen, but it also holds up Mary Alyce, who does not like to be kept waiting.</p>

<p>Optimistic concurrency control says, "We KNOW there could be a race condition, but we'll add a very cheap way to detect if it occurred. And if so, we'll pay the very expensive cost of restarting one of the transactions from the beginning."</p>

<p>The only changes from the above sequence are in the last two lines:</p>

<blockquote>Bob's ATM updates the main database with the new balance ($1200)</br>
Mary Alyce's ATM updates the main database with the new balance ($1300)</blockquote>

<p>These become:</p>

<blockquote>Bob's ATM updates the main database with the new balance ($1200) so long as the current balance is still $1000.<br>
Mary Alyce's ATM updates the main database with the new balance ($1300) so long as the current balance is still $1000.</blockquote>

<p>That's easy to write but trickier to implement. The check of the balance and the update of the new balance must occur within the microprocessor as an atomic action. It all must happen basically as one single operation.  Some processors have had these instructions for a long time, but they're pretty common now and called "test-and-set" or "compare-and-set" instructions.</p>

<p>If the "so long as" part fails, the whole transaction must be restarted from scratch. In our example, Mary Alyce's $300 would pop back out of the machine and she'd have to start over.</p>

<p>That's very expensive for Mary Alyce, but the actual occurrence of the race condition is very rare. So although the redo is expensive it hardly ever happens, so no one has to wait for another person doing an update operation.</p>

<p>Apply this to 25,000 ATMs and suddenly the database is decoupled from transaction processing and the system is additionally controlled for internal race conditions such that it can run with less code and at full speed, which is saying something.  Suddenly the system can be 100 times faster (cascading 10X improvements) or run 10 times faster on one tenth the hardware (take your pick), all thanks to the timely embrace of clever multi-threaded programming.</p>
]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081013_005498.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081013_005498.html</guid>
			<category></category>
			<pubDate>Mon, 13 Oct 2008 19:50:04 -0500</pubDate>
		</item>
				<item>
			<title>Off With Their Heads!</title>
			<description><![CDATA[<p>My promised column on threads will appear in this space on Friday. It would have appeared here today but the crumbling global financial system suddenly seemed a more appropriate topic.</p>

<p>We're in trouble and by "we" I mean the whole darned planet. What started as a mortgage problem in the U.S. has blown into global financial paralysis that threatens us all with recession and maybe even with depression. I know I'm feeling depressed, how about you? The crisis seems immune to any and all efforts to fix or end it. NOT passing a $700 billion mortgage bailout can send Wall Street into a tailspin, for example, but then finally passing the bailout didn't seem to improve things, either. The Federal Reserve and Treasury Department are running out of tools and time yet still the system flirts with suicide. So I say it is time to take a completely different view of the problem and to look to a new leader to solve it, in this case Jack Welch.</p>

<p>Jack Welch is the retired chairman and CEO of General Electric who took the company during his 23-year tenure from being worth about $14 billion to about $410 billion by really MANAGING the business and concentrating on creative use of capital. I've written columns and columns deriding managers as a profession but none of that applies to Jack Welch and GE, where managers really manage -- they manage the heck out of the place and to generally good effect. Jack Welch built that system, he has time on his hands, I say let's give him a new job.</p>

<p>There is very little difference, in fact, between the global financial system and General Electric. Welch saw GE entirely in terms of cash flows and the application of capital to those parts of the business where it would do the most good. Welch also thought in terms of continuous quality improvement, which is virtually unknown on Wall Street OR in Washington, where such things aren't even talked about, much less measured.</p>

<p>I've been thinking about this crisis and a lot of it comes down, I believe, to a fear of failure especially on the part of the banks. There is no credit available to anyone, anywhere, no matter what the credit rating or score. This is because the banks are frozen by fear to the point where they won't even lend to each other much less to customers. This fear of failure seems to be pretty much guaranteeing failure. And the regulators are now throwing what will soon be trillions of dollars at trying to break these bankers out of their paralysis. But I think there is a better way: use this very fear of failure as a motivator.</p>

<p>Before we get to Jack let's deconstruct this current psychological crisis on the part of the banks. They aren't lending money because they are afraid it won't be repaid. They won't lend even to each other because banks seem to be failing all over yet there hasn't been an instance yet when an overnight loan has resulted in default. So what's the problem? More properly, what is the outcome the banks fear?</p>

<p>They fear going out of business either through honest failure or through being forced to merge or having their deposits taken away by the Federal Deposit Insurance Corporation (FDIC). In short it comes down to fear of losing their licenses, because as a highly regulated industry the banks can only do business at all with the permission of government. Right now they are totally fixated on the idea that if they lend money and it isn't repaid the government will pull their licenses. Yet the government has made it clear that the most important thing is to LEND MONEY, breaking this credit paralysis. All the banks know this but none of them want to be the first to take the big risk of lending money.</p>

<p>You do it! No, you!!</p>

<p>Enough of this crap. What if Jack Welch was the U.S. banking czar? We know the result of all such crises these days in the U.S. is the appointment of a czar -- a new government official drawn from industry and charged with cutting across agency lines and streamlining an ultimate solution. We did it in energy and terrorism and I'm sure we'll do the same thing now so let's just think ahead a bit. Let's assume that's the case here and that whatever President is in office names Welch. What would Jack Welch do?</p>

<p>If Welch ran the U.S. banking system like he ran GE, he'd kill the bottom 10 percent of banks every year and fire the lowest 10 percent of bankers.</p>

<p>What impact would that have on the system? Would it make it better or worse? Well it couldn't be worse, could it?</p>

<p>Doctors have a way of measuring pain. It seems our brains can only focus on one pain source at a time, so if you have a pain in your gut you really don't notice the pain in your ankle. So there is a device called a palpometer that goes on your arm or leg and is calibrated to produce standardized and replicable levels of pain. Put this gizmo on, turn up the pain, and when you get to the point where the patient suddenly goes from saying, "My head hurts," to "My leg hurts," then you know how much pain they are in. </p>

<p>This was all pre-waterboarding mind you.</p>

<p>Right now all the bankers are afraid to lend money because they are afraid of failure. As the new banking czar Jack would much rather have them be afraid of HIM. If the bottom 10 percent of bankers were fired every year and the bottom 10 percent of banks had their branches and deposits redistributed, wouldn't they be more afraid of THAT than of making bad loans? Their motivation would still be to make GOOD loans, but the penalty for making NO loans would be there, too.</p>

<p>Our problem then is that we're throwing money at something we should be handling using a different regulatory tool -- licensing.</p>

<p>U.S. bank regulators should go to all the banks this afternoon and say, "You aren't making loans, which is part of the definition of what it is to be a bank. If you aren't acting like a bank by tomorrow we'll take away your banking license and transfer your deposits to another bank that WILL make loans."</p>

<p>Problem solved overnight.</p>

<p>It's only one part of the problem, of course, but this solution will cost a lot less than $700 billion. It will cost nothing.</p>

<p>And if you think it won't work, then you don't know Jack.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081008_005497.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081008_005497.html</guid>
			<category></category>
			<pubDate>Wed, 08 Oct 2008 11:56:08 -0500</pubDate>
		</item>
				<item>
			<title>Data Debasement</title>
			<description><![CDATA[<p>Last week I was in Boston to moderate a panel at the <span class="caps">MIT</span> Technology Review&#8217;s Emerging Technologies Conference &#8212; one of those tech shindigs so expensive I can only attend as hired help.  My panel was on parallel computing and it produced this column and another I&#8217;ll file early next week.  This week is about databases and next week is about threads.  Isn&#8217;t this a grand time to be a nerd?</p>

<p>Thanks in part to Larry Ellison&#8217;s hard work and rapacious libido, databases are to be found everywhere.  They lie at the bottom of most web applications and in nearly every bit of business software.  If your web site uses dynamic content, you need a database.  If you run <span class="caps">SAP </span>or any <span class="caps">ERP </span>or <span class="caps">CRM </span>application, you need a database.  We&#8217;re all using databases all the time, whether we actually have one installed on our personal computers or not.</p>

<p>But that&#8217;s about to change.</p>

<p>We&#8217;re entering the age of cloud computing, remember?  And clouds, it turns out, don&#8217;t like databases, at least not as they have traditionally been used.</p>

<p>This fact came out in my EmTech panel and all the experts onstage with me nodded sagely as my mind reeled.  No database?</p>

<p>No database.</p>

<p>Parallel computing used to mean scientific computing, where hundreds or thousands of processors were thrown at technical problems in order to solve them faster than Moore&#8217;s Law might otherwise have allowed.  The rest of us were relying on rising clock rates for our performance fix, but scientists &#8212; scientists with money &#8212; couldn&#8217;t wait so they came up with the idea of using multiple <span class="caps">CPU</span>s to solve problems that were divided into tasks done in parallel then glued back together into a final result.  Parallel computing wasn&#8217;t easy, but sometimes that was the whole point &#8212; to do it simply because it was so difficult.  Which is probably why parallel computing remained a small industry until quite recently.</p>

<p>What changed was Moore&#8217;s Law put an end to the clock rate war because chips were simply getting too hot.  While faster and faster chips had for the most part linear performance increases along with cost and power consumption decreases, the core temperature inside each microprocessor chip was going up at a cubic rate.  Back in 2004 Intel released a chart showing that any clock speed over 5 GHz was likely to melt silicon and Moore&#8217;s Law would, by 2010, make internal processor temperatures similar to those on the surface of the Sun!</p>

<p>For those, including me, who think that&#8217;s pretty darned hot I&#8217;ll point out one of my astronomer readers immediately had to mention that the Sun&#8217;s chromosphere is actually much hotter than the surface. Forgive him, he means well.</p>

<p>Faced with this absolute thermal performance barrier, Intel and <span class="caps">AMD </span>and all the other processor companies had to give up incessant clock speed increases and get us to buy new stuff by putting more than one <span class="caps">CPU </span>core in each processor chip can.  Now chips with two and four processor cores are common and Intel hints darkly that we&#8217;ll eventually see hundreds of cores per chip, which brings us right back into the 1970s and &#8217;80s and the world of parallel computing, where all those principles that seemed to have no real application are becoming very applicable, indeed.</p>

<p>And that&#8217;s exactly where databases start to screw up.</p>

<p>Bob Lozano, chief visionary, evangelist, father-of-eight (same woman) at Appistry came up with the first database example I&#8217;d heard and it was eye opening.  Appistry (I&#8217;ve written about them before &#8212; it&#8217;s in the links) specializes in distributing what would normally be mainframe applications across tens, hundreds, or even thousands of commodity computers that act as one.  If a government agency wanted to do meter-scale analysis of very high-resolution images from spy satellites, it might use Appistry, I&#8217;d imagine (just a guess, of course). </p>

<p>The database example Bob used at <span class="caps">MIT </span>was of an unnamed customer that is a credit card transaction processor.  Their core application &#8212; the processing of credit card transactions &#8212; was happening on <span class="caps">IBM</span> Z-series mainframes (the biggest of big iron) and the client wanted to port the whole mess to commodity PCs. </p>

<p>Google did it, why not them?</p>

<p>But the first time they tried it at Appistry, it didn&#8217;t work.</p>

<p>&#8220;We hired a technical team that had done similar applications before so they started with replicating the mainframe architecture on the commodity computers, database and all,&#8221; Lozano recalled.  &#8220;But when they finished it wasn&#8217;t appreciably faster than the mainframe it replaced.  Even worse, it wouldn&#8217;t scale.  So we fired the team and started over.&#8221;</p>

<p>The problem, it turned out, was in the database. </p>

<p>The way the original mainframe application functioned was by first receiving transactions, writing them to the database, reading them from the database, then doing the actual processing before writing them again back to the database.  That&#8217;s read-write-read-process-write.</p>

<p>The second time through the Appistry team tossed the database, at least for its duties as a processing platform, instead keeping the transaction &#8212; in fact <span class="caps">ALL </span>transactions &#8212; in memory at the same time.  This made the work flow into read-process-write (eventually).  The database became more of an archive and suddenly a dozen commodity PCs could do the work of one Z-Series mainframe, saving a lot of power and money along the way.</p>

<p>If this sounds like a risky way to do business &#8212; not writing the data to disk until things slow up enough &#8212; remember that&#8217;s the way Google runs its search engine and why it is so darned fast.  Google has <span class="caps">THE ENTIRE INTERNET</span> IN <span class="caps">MEMORY</span> AT <span class="caps">ONCE. </span> If the application slows down they just add more hardware.</p>

<p>This is good news for cloud computing and bad news for mainframes, because systems like Appistry and its competitors (there are several) are going to eventually bury the mainframe by putting the &#8220;cloud&#8221; into cloud computing.  Suddenly a Storage Area Network with a relatively weak database controller is good enough for archiving while the parallel or even massively parallel cloud does the real work.</p>

<p>Later this month Microsoft will announce a cloud version of Windows Server and it will be very interesting to see how it handles database integration and dis-integration.</p>

<p>The database problem is much more than just slow reads and writes. Relational databases also create false dependencies between pieces of data. Dependencies of any kind break parallelism, and therefore make an application hostile to commodity platforms. That is, if one chunk of data (A) is dependent on another chunk of data (B), then no work can be done on A until all work on B is complete. If the dependency is real, like when A and B are both withdrawals from the same bank account, then there are hacks we can try like one I will describe in my next column, but most programmers just choose to have a cup of coffee and wait for B to finish.</p>

<p>But if these are withdrawals from different bank accounts, or maybe even different banks, then no true dependencies exist. Unfortunately, if all of this has been stored in a single relational database then we unintentionally create a false dependency, since that database can only handle a fairly limited amount of items concurrently &#8212; we&#8217;ve created a bottleneck that will choke the application.</p>

<p>While the database guys are busy figuring out how to add more and more concurrency internally, in reality when you take a few steps back and think of a large set of commodity boxes all executing a single data munching app, then no matter how sophisticated we get, the relational database will still effectively be a single thread to that app.</p>

<p>A traditional response is to pour dollars into the data tier, buy faster, more concurrent <span class="caps">SAN</span>s, better interconnects, and bigger database servers. That works up to a point and pleases Sun and <span class="caps">IBM </span>no end.</p>

<p>Somewhat more helpful though are data grid products like Coherence or eXtreme Scale (IBM), or Appistry&#8217;s Fabric Accessible Memory (FAM). For many applications those can take more of the lifetime of each chunk of data and keep it in memory in the middle tier &#8212; hopefully on the same boxes where this large data-munching app resides.  But this still doesn&#8217;t completely solve the problem because there remain limits on how far the relational database can scale.</p>

<p>Here&#8217;s how Google attacks this problem, which goes beyond simply keeping the entire Internet in memory.  The problem, of course, is that you can&#8217;t keep the entire Internet in memory in every server because then you&#8217;d need more memory chips than even Google can afford to buy. </p>

<p>To scale the Google search service, then, they figured that many large problems did not intrinsically require doing actions one at a time. But Google first had to free itself of the false dependencies.  So they coined the term MapReduce and created both a set of operations and a way to store the data for those operations natively, all while preserving the natural independence that is inherent in each problem, building the whole mess atop the remarkable Google File System, which I&#8217;ll cover some other day.</p>

<p>Google led the way but many other companies have followed suit, opening doors to a wide range of new ways of thinking about large-scale data manipulation. Suddenly there are different ways to store the data, new ways to write applications, and new places (thousands of cheap boxes) to run such applications.</p>

<p>What this does for Larry Ellison and his libido is a great question, because it looks like he&#8217;s bought up most of the traditional database-centric software industry just in time for it to be declared obsolete.</p>

<p>Sorry Larry.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081003_005496.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20081003_005496.html</guid>
			<category></category>
			<pubDate>Fri, 03 Oct 2008 15:20:13 -0500</pubDate>
		</item>
				<item>
			<title>The Cringely Plan</title>
			<description><![CDATA[<p>In the early 1980s I was a volunteer firefighter for a tiny community in the Santa Cruz Mountains of Northern California.  We all lived in a beautiful redwood forest and our task was to keep that forest from burning down in a huge conflagration, taking us all with it.  The job was made all the harder because our little part of paradise hadn't burned since the 1920s, so there was 60+ years of flammable undergrowth just waiting to light off.  The current financial crisis facing the United States and the world really isn't much different from that.</p>

<p>An unmanaged forest, one without the sort of fire control we attempted to provide, would naturally burn every few years.  The undergrowth would build up, reach a critical mass, some source of ignition would come along -- usually lightning -- and all that undergrowth would burn. The redwoods themselves would be scarred but not really threatened, as we could see from the charring that marked them from countless such fires over centuries.  Of course burning undergrowth threatened homes and property, too, so there was a natural desire on the part of that community to want the next burn to not come this year, please not this year.  So there came a policy of aggressively fighting fires with the result that we eventually faced 60 (now 90!) years of flammable material growth rather than six or eight years.  And the probable fire fueled by 60 years of undergrowth would have been so bad that our job changed to one of trying to prevent fires from happening, well, ever.  This was an impossible task, of course.  Eventually the stars would align the wrong way and the whole place would burn down, we all knew it.  Just let it not happen on our watch.</p>

<p>Does this sound familiar?</p>

<p>Now America and much of the world face the possibility of recession and we handle that by first arguing about the definition of the term.  Are we or are we not in recession?  This distinction appears to be very important to some who view it like a forest fire: is it burning or not?  Implicit in this distinction, I suppose, is the idea that if we're not burning -- if we are not in recession -- that maybe through some miracle we'll never face that problem.  Whether this is a practical attitude or not depends entirely on your event horizon -- how soon you expect things to change.</p>

<p>The people in power in this country have a relatively short event horizon.  Politicians tend to think of two, four, or eight years as the longest periods of time that matter.  Corporate honchos might look out further, you could guess, but they don't since the average duration for a U.S. Fortune 500 CEO is under four years.  So while the intent of the fire chief and the mayor and the governor and the Congressman and the President and the CEO is that there be no unpleasant surprises during their watch, all of them know such surprises are coming.</p>

<p>This short-term focus in the face of long-term difficulties leads to odd behavior at times.  In the forest it is usually better to let smaller fires burn or even to deliberately set them, yet few fire chiefs are willing to take that risk, even though NOT taking the risk is so much worse.  In financial markets, as companies crumble and governments prepare bailouts, short sellers pile on in scrums of doom that make the shorts rich yet hurt society.  Traders with a trading mentality, they can't help themselves any more than Ralph Nader can keep from running for President.  "But George Soros did it to the Bank of England," they say, as if that makes everything okay.</p>

<p>The American economy is at the end of its longest-ever period without a recession.  Through sleight of hand and a fair amount of financial fraud we've managed to keep the "R" word out of our communal vocabulary for 14+ years.  We did this through a succession of bubbles -- first the Internet bubble and then the real estate bubble with a little war bubble thrown in between.  Financial bubbles are unnatural enough in themselves, but piling bubble atop bubble defies logic, yet still we managed to make it happen, helped somewhat by half a trillion dollars in war spending, all with borrowed money.</p>

<p>By rights the end of the Internet bubble should have sent us into recession, burning the financial undergrowth and setting up the next period of prosperity.  But we avoided that and the result is what we see now -- something far worse.</p>

<p>Recession is inevitable at this point, yet still we talk about avoiding it.  This is crazy talk.  If we as an economy somehow push the next recession back a few more years, it will be all the worse when it comes.  AND IT WILL COME.  Everybody in power knows this; none of them argue against it; they all know recession is inevitable; they know that forestalling a recession at this point will only make it worse.  Yet still they pray, "Not on our watch, please."</p>

<p>So we plan $700 billion bailouts on top of hundreds of billions in other knee-jerk measures all intended simply to forestall the inevitable and therefore ultimately bound to fail.  "Just not on our watch, please."</p>

<p>The cost of delaying the inevitable is high, but we won't pay it, our children and grandchildren will.  We can argue policy all day and whether this matters or not, but it is hard to argue that any such cost doesn't matter IF IT DOESN'T WORK.</p>

<p>So I propose The Cringely Plan -- a relatively small attempt to point public policy in a direction that makes sense for a change.  The Cringely Plan won't avoid a recession because that's impossible.  The Cringely Plan is intended, rather, to look beyond the inevitable recession and assist with the ultimate recovery and beyond.</p>

<p>What we need are energy and economic policies that play to the strengths of government, not its weaknesses.  Governments are good at big moves that force changes of direction, not little moves intended to, for example, create the economic "soft landing" we'll start to hear about in a couple months.</p>

<p>Governments are best at turning super tankers, not at docking them.</p>

<p>And while governments can print money and thereby pay for a lot of stuff, they are actually most effective just telling us what we can or can't do, more than anything else.   We can see that, for example, in the Clean Air and Clean Water Acts of the 1960s that significantly improved the lives of all Americans without the government having to pay for it.</p>

<p>The Cringely Plan does nothing for banks or mortgages.  That's a problem that will have to work itself out, I think, and will if we give it a chance to do so.  But we won't give it such a chance, I'm guessing, because it is an election year and because government is viewed as stupid and able to be threatened into paying for the most amazing things that it shouldn't.  So The Cringely Plan has to look past banks and mortgages to energy policy and economic stimulus.</p>

<p>It would be interesting to know what the mortgage market would be like had the price of a barrel of oil not hit $140+.  There still would have been a mortgage bubble bursting, but I wonder when?</p>

<p>Whatever the banking situation, though, we still need an energy policy and simply have not had one for decades.  So get ready, here it comes; The Cringely Plan calls for:</p>

<p>Prohibiting the manufacture and sale of incandescent lights.</p>

<p>Kind of a letdown, eh?</p>

<p>That's it.  No ethanol subsidies, no drilling in wilderness preserves, no tax credits, no artificial price supports, no enriching good ol' boys from the oil patch.  And no cost to government at all.</p>

<p>As our incandescent lights wear out we'll have to replace them with something else, primarily compact fluorescents and LEDs.</p>

<p>Yes, this will lead to the closure of light bulb and filament plants in Ohio and elsewhere, costing a few thousand jobs.  At the same time it will create jobs in the non-incandescent light industries as those ramp up.  But most importantly, it will within a year (based on a 700-hour life for incandescent bulbs) lead to an 18 percent drop in U.S. electrical demand.</p>

<p>Dropping electricity demand by 18 percent will have an interesting effect on the electric utility industry.  A simplistic view would suggest that our electric bills should drop by 18 percent.  Cynics will point out that fixed overhead built into the regulated industry will keep bills from dropping that much simply because doing so would unduly hurt utility profits.</p>

<p>I don't think so.</p>

<p>Our utility bills pay for power that comes from many sources, some more efficient or profitable than others.  Our bills also pay for building new power plants, some of which are again more efficient or profitable than others.  An 18 percent cut in demand will have a huge impact on the production strategies and capital spending plans of every U.S. utility.  More expensive plants built to serve marginal demand -- plants like smaller gas turbines for example -- could be taken off-line, dropping the average cost per kilowatt.  Large new plants that have been in the works (and that we've been paying for) for years can be eliminated or delayed.  Cancelled plants would lead to higher profits that lead to lower electrical rates.  So our electric bill won't go down 18 percent, they'll go down 25 percent, which is a savings of $22 per month for the average American home.</p>

<p>Twenty-two dollars per month!?  Big deal.  Was there an economic component of this energy policy?</p>

<p>Well, it's a savings of $29 billion per year EVERY YEAR FROM NOW ON, which pales the recent and easily forgotten economic stimulus package that sent $600 checks generally not to the people who needed them most.  How much of a positive effect those checks had on the economy is open to debate, but how much effect they had on reducing energy consumption isn't, because that effect was nada, zilch, zero.</p>

<p>$29 billion also amounts to about the annual debt service cost of either the Iraq war or the coming $700 billion bailout.  Take your pick.</p>

<p>Want another energy policy with positive economic implications?  Make single-phase lighting illegal for businesses with ceiling heights above 15 feet.  Three-phase power at 380 or 440 volts is more efficient, requires less copper, and saves money overall.  Once you have it in place for lighting it's a no-brainer to also use it for electric motors, where the savings are HUGE.  The financial payback period for such conversion is under two years, which is an imputed annual interest rate of 50 percent.  What investment can you reliably make that pays back 50 percent year after year after year?</p>

<p>Still no government money has been spent.</p>

<p>Take this a step further and require all new residential construction to use 380V three-phase power for EVERYTHING, which is to say turn the U.S. into Europe, where homes already use less power for the same level of service because of three-phase efficiency.  Now this is a big change, of course, because it means getting all new electric appliances, TVs, everything.  What's wrong with that?  It's not like the products don't exist, since the same manufacturers are already making them for sale in Europe.  Market expansion leads to lower prices.  Swapping out 10 percent of the U.S. appliance market per year ON TOP OF NORMAL ATTRITION would create a huge boom in electronics and electrical goods.  Okay, maybe this one will cost some money, but no more than those $40 digital decoder subsidies the government is already handing out.</p>

<p>These are just a few examples of what can thoughtfully be accomplished. There's a natural role for government here and that's setting rules. They are good at that. Government hasn't always been as good at enforcing rules as setting them, but that doesn't mean rules aren't worth having.   Let's just set a few new ones that make sense as we recover and rebuild after the economic fire to come.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080926_005422.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080926_005422.html</guid>
			<category></category>
			<pubDate>Fri, 26 Sep 2008 23:15:59 -0500</pubDate>
		</item>
				<item>
			<title>Door Number Three</title>
			<description><![CDATA[<p>I&#8217;ll begin this third and (I promise) last column on IT management with a confession: I have been fired from every job I have ever held.  This is certainly not something I set out to do, nor did I even realize it until one day my young and lovely wife mentioned that I had never told her about voluntarily leaving any position.  It&#8217;s not that I&#8217;ve had so many jobs, either.  This one and the one before it have kept me going for more than 20 years.  But they always seem to end the same way. This one might, too.  You can never tell.</p>

<p>Most of the times I have been fired it&#8217;s because I&#8217;ve been judged to be unmanageable, which is to say I won&#8217;t shut up.  The ultimate reason given is usually something minor.  The last time around, for example, I was fired because I didn&#8217;t transfer the cringely.com domain to my employer.  They asked me to do it and I said &#8220;no.&#8221;  Had they said, &#8220;Transfer the domain or you will be fired,&#8221; I might have decided differently.  But they never said that &#8212; never gave a hint of the consequences &#8212; so I assume the real goal was less to get the domain and more to get rid of me.</p>

<p>The guy who had me fired, Stewart Alsop (maybe you&#8217;ve heard of him), ultimately lost his own job for firing me, at least according to International Data Group Chairman Pat McGovern, who told the story to 300 people once at a <span class="caps">DEMO </span>conference.</p>

<p>Back when I was a kid and working at <span class="caps">WWST</span> Radio in Wooster, Ohio, I was fired for writing those seven unspeakable words in the middle of a livestock auction report.  Another kid had been playing similar tricks on me for weeks, but when I finally retaliated he turned me in.  I guess I was a threat to him and didn&#8217;t know it.</p>

<p>One company hired and fired me three times and another company hired and fired me twice.  And somehow in all this I&#8217;ve never received severance or unemployment compensation.  I just found another job or it found me.</p>

<p>There&#8217;s a point to all these firing stories and they actually do relate to <span class="caps">IT. </span> I&#8217;m typical of a lot of IT types.  You know us.  We are useful but sometimes a pain in the ass.  We have opinions and speak our minds and don&#8217;t suffer fools at all.  We stand up to authority from time to time.  Sometimes we&#8217;re wrong.  We get fired a lot and hired a lot, too, because we are generally useful, though dangerous.</p>

<p>What do you do with folks like me if you are a manager?  At a traditional newspaper you&#8217;d either fire me or make me a columnist.  And, sure enough, look where I am.  In an IT shop you give me a task to do and let me do it, generally on my own, and never <span class="caps">EVER </span>put anyone under me, because I am hopeless as a manager.</p>

<p>But it turns out I&#8217;m not so bad as a leader.</p>

<p>Weird, eh?</p>

<p>The last two columns have shown that IT is the Cousin It of American industry.  We serve the company but often don&#8217;t feel part of it.  Certainly the value structures and lines of authority that function perfectly well for most of the rest of the company don&#8217;t work at all well for <span class="caps">IT. </span> We&#8217;re vital but at the same time, well, so different that it&#8217;s hard to imagine a <span class="caps">CEO </span>emerging from the IT ranks.  It happens from time to time.  Everyone points to John Reed, who rose from IT to <span class="caps">CEO </span>of Citicorp, but Reed was an exceptional case.  He succeeded because his predecessor, Walter Wriston, had an unusual interest in IT and mentored Reed.  Reed succeeded, too, because he didn&#8217;t really come from IT but from Data Processing, which was more hierarchical.  And ultimately he didn&#8217;t succeed at all, by some measures, because John Reed was fired.</p>

<p>So right now let&#8217;s just accept that it is very unlikely that, coming from <span class="caps">IT, </span>you&#8217;ll ever become the <span class="caps">CEO </span>of your company.  That means you are instantly off the traditional management track and have the option of either eventually moving on to some other organization or taking what&#8217;s behind Door Number Three.</p>

<p>Remember Door Number Three from Let&#8217;s Make a Deal?  It could reveal a sports car or a donkey, but whatever was behind Door Number Three was unlike anything you could imagine.</p>

<p>We need a Door Number Three for IT professionals.</p>

<p>I have a friend of 20 years who is in a key technical role at a very large company.  He&#8217;s too vital to the company to risk losing but too geeky to fit in.  He&#8217;s on the craft (non-management) salary scale, but way higher than he ought to be for having no direct responsibility.  All he does, in fact, is from time to time save his company from ruin.  And even more rarely, he saves all the rest of us from ruin, too, in ways I am not at liberty to explain.  How do you manage such a guy?  Where he works they have him report to the <span class="caps">CEO. </span> The Big Guy has 5-6 direct reports and one of them &#8212; my friend &#8212; doesn&#8217;t manage anyone or anything.</p>

<p><span class="caps">THAT&#8217;S</span> Door Number Three.</p>

<p>We&#8217;re in an important transition period not just for <span class="caps">IT, </span>but also for business in general.  Everything seems to be in flux.  And that means the old ways of doing things are changing and ought to.  And in this way IT is leading &#8212; or ought to lead &#8212; the way.  Later this week I&#8217;ll be making a dramatic shift and proposing the Cringely Energy/Economic Policy, but first I need to drive home the point that, however different it is from the rest of the company, IT is generally the vanguard for a new corporate culture and whole new ways of doing business for the world.</p>

<p>We&#8217;re in a mess.  The world is screwed up and some of that can be traced to the improper use of IT as a financial weapon.  But the people of IT actually present many of the answers we need, because they are living much deeper in technology than other parts of the company or of our society.</p>

<p>Think about it.  There has nearly always been a class of eggheads showing us a path toward new business models, whether it was Edison and Firestone, Hewlett and Packard, Noyce and Moore, Gates and Allen, or Brin and Page.  It takes in each case a generation to happen, but ultimately we all (and I mean <span class="caps">ALL </span>&#8212; everyone in the total organization) come to look like the geeks of the generation before.  So let&#8217;s lean into that, get on with the transition, and get past this place we&#8217;re in right now where nobody wants to be.  Let&#8217;s consciously embrace the next model that&#8217;s generally running fitfully right now inside every company, down in the more functional parts of the IT department.</p>

<p>What I mean by this is that times have changed and the world can no longer afford even John Reed&#8217;s world view with its needs analysis, design, debug, test, rollout strategy &#8212; whether we&#8217;re talking about a new app or a new marketing campaign.  By the time the app (or the campaign) is rolled out, the world changed from <span class="caps">HTML </span>to  Javascript/SOAP/Ajax (or from financial regulation is bad to financial regulation will save us). </p>

<p>At the heart of this is a concept completely foreign to traditional business &#8212; Open Source.  What the open source community has demonstrated is the superiority of a strategy that emphasizes early proof of concept, early release, and frequent releases with features added as needed &#8212; probably totaling 20 percent of the features identified in a needs assessment. </p>

<p>This is the new IT strategy we live with every day &#8212; 80 percent solutions because they are fast, increasingly reliable, and keep the end users in the loop from almost the beginning.   All made possible because of an open Internet (at least until Comcast succeeds and enslaves us), easily grasped standards and impressive demonstrations by companies like Amazon, Google, Facebook, and a ton of start-ups. Wall Street back offices figured this out long ago, they just never got their boss&#8217;s bosses to understand.</p>

<p>Last week&#8217;s column was a utopian vision that simply requires all the old managers to be reprogrammed or accept a bullet in the head.  But it is not at all utopian if applied solely (or initially) to <span class="caps">IT, </span>where this stuff actually works pretty well. </p>

<p>IT people are most of the time building fortresses or feeling unappreciated &#8212; often both at the same time.  Yet to our discredit, we&#8217;ve done a very poor job of explaining or demonstrating or outright selling our utility to the broader organization.  Where are our Geek Appreciation Days?  Take a Geek to Lunch?  Bring Your Geek to School?  Taciturn, we disparage our co-workers for not appreciating us while giving them little obvious reason why they should appreciate us.</p>

<p>That has to change.</p>

<p>Door Number Three isn&#8217;t just an escape hatch for nerds, it is the way business and culture and civic life will be for most of us a generation further into this information age.  We&#8217;re just leading the way.  And if we&#8217;re leading the way let&#8217;s embrace that role and become leaders.</p>

<p>If, like me, you are likely to be fired, anyway, there&#8217;s no real downside to this strategy.  Let&#8217;s give it a try.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080922_005421.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080922_005421.html</guid>
			<category></category>
			<pubDate>Mon, 22 Sep 2008 14:29:57 -0500</pubDate>
		</item>
				<item>
			<title>Leadership</title>
			<description><![CDATA[<p>Last week&#8217;s column on bad IT management and the strong response from readers that followed show this to be a huge issue.  There are <span class="caps">WAY </span>too many IT managers who either can&#8217;t or shouldn&#8217;t manage technical teams.  Last week I maintained that having a firm technology base, or at least the ability and willingness to acquire one, was essential for good managers.  While readers got carried away with which technical test is the best, I don&#8217;t think there is much dispute that there are certain aspects of technical management that are helped by the manager being a code god.  But that&#8217;s far from all there is to the job.  So this week I want to go deeper and look at what&#8217;s really missing in nearly every instance of such bad management, which is leadership.</p>

<p>The distinction between management and leadership is a critical one.  Management is &#8212; at its very best &#8212; an exercise in coping while leadership is so much more.  Last week&#8217;s simple idea that the manager ought to at least be able to tell good work from bad is exemplified by Bill Gates, who liked to claim that he could tell good code from across the room and that whatever task a team was facing was something he could code in Visual Basic over a weekend.  Both statements are nonsense, of course, but Bill knew he had to talk the talk, making him at least an adequate manager.</p>

<p>Does it make him a leader?  I don&#8217;t think so.  But let&#8217;s not blame Bill for that.  Let&#8217;s blame Charles Simonyi.</p>

<p>Charles is the guy who came up with Microsoft&#8217;s development process &#8212; an outgrowth of his research at Xerox <span class="caps">PARC. </span> I covered this extensively in my book, Accidental Empires, but the short version is that Charles came to advocate a strong program manager as the central controller of any development group.  One person made all the decisions and as long as that one person was correct 85 percent of the time, it was better to have a dictatorship than a democracy or even a meritocracy.  This was an effective way to extend Bill&#8217;s will to Microsoft programmers Bill would never even meet.  And to Charles&#8217; credit the system worked well enough if the dictator was really, really smart and the task at hand wasn&#8217;t too complex.  It was perfect for the 1980s.</p>

<p>But it is far from perfect today and represents one of the fundamental reasons why Windows Vista was so late to market and such a mess when it finally shipped.  Vista had plenty of management, but not very much leadership.   </p>

<p>When I think of leaders what comes to mind first are political and military leaders.  We use the term &#8220;leadership&#8221; to describe those roles far more than we do for what ought to be similar roles in business or technology.  This week former Hewlett-Packard <span class="caps">CEO</span> Carly Fiorina said that John McCain, Barack Obama, Sarah Palin, and Joe Biden were all ill-suited to be <span class="caps">CEO</span>s of major corporations.  However badly the statement went over (Carly supports McCain, by the way), her real point was that there are different skill sets for leaders than managers.  And she&#8217;s right to an extent, but it also says a lot about her own tenure at H-P, which was long on management and short on leadership.</p>

<p>Management is telling people what to do, which is a vital part of any industrial economy.  Leadership is figuring out what ought to be done then getting people to do it, which is very different.  It is a vital part of any successful post-industrial economy, too, but most managers don&#8217;t know that.</p>

<p>Let&#8217;s use the <span class="caps">U.S. </span>military involvement in Iraq as an example of the difference between leadership and management.  As more books are written and stories come out we can see that there is a lot of arguing that goes on inside the military.  Officers are onboard or not.  They are proposing various strategies and taking positions generally advocating what&#8217;s perceived as safest for both the mission they have accepted and the preservation of life among their troops.  Eventually someone makes or imposes an order, but even then there is a lot of second-guessing in the military, which has to be ready with Plans B through G just in case they are needed.  This is a lot different from the image many of us have of General Patton pointing toward Berlin, imposing a singular view and forcing it through.</p>

<p>In contrast to the military, most businesses do a lot less explaining and pondering and a lot more laying down edicts.  That&#8217;s management, which works fine on an assembly line, but not at all well building a big software application or winning a war.</p>

<p>Janna Raye is someone I worked with at InfoWorld half a lifetime ago who has built a consulting career on understanding this stuff and helping companies to transcend 20th century corporate hierarchies and become what she calls &#8220;fractal organizations.&#8221;  Janna&#8217;s consulting business is called Strategems and here is her take on this issue:</p>

<p>&#8220;Modern corporations suffer from systemic-level issues that emerge in top-down hierarchies. Managers are there to control staff and budgets, not to lead. Although you can make valiant and often successful attempts to control things and processes, you will never again be able to control people. We&#8217;ve evolved, basically, and the information age has had a lot to do with it. So we still &#8220;manage&#8221; companies the same way as when we actually operated assembly lines in America&#8212;the good old days! Now, people need leaders, not managers, and that&#8217;s what a fractal organization enables.</p>

<p>&#8220;In fractal organizations, it&#8217;s the staff deciding how to continuously improve processes in their functional areas for efficiency of time and resources. These organizations thrive with a new pay model also, based upon results or value of work delivered and not how much time it takes to do the task. Those who are really good will get to go home early! These are not the organizations that are shrinking. Like galaxies, they continue to expand, actually aided by a strong gravitational pull of the leaders at the center. Those who do it well create a compelling vision and keep it alive. They allocate resources to projects that align with the vision, and reward arm- and team-cluster leaders for the creative ideas their staff bring to the organization.  It&#8217;s a shared vision and collective goals that are missing from the vast majority of organizations, which is why failing projects continue to drain resources. Really caring about what you do and feeling proud to be a part of something special and wonderful is what every human desires, even if they say they don&#8217;t.&#8221;</p>

<p>So what&#8217;s Janna&#8217;s model for the ideal 21st century organization?  Pixar.</p>

<p>&#8220;They let creativity run wild at Pixar!&#8221; says Janna.  &#8220;Ed Catmull, Pixar&#8217;s president, wrote an article for the Harvard Business Review on their collective creativity. Ed and John Lasseter (and sometimes Steve Jobs) are in the center of the galaxy, keeping the gravitational pull strong and the company rotating, so to speak. Around them are the directors, who in the fractal org model lead the arms (no more divisions!). Each film team or cluster, from the storyboard artists to the renderers, goes through the iterations necessary to achieve the best results. Everyone is on board with creating exemplary films and they are relentless in demanding only the best. But in this process, as they say, they have to get all the &#8220;sucky&#8221; ideas out first. If they don&#8217;t reveal all the ideas, they don&#8217;t get all the perspectives and therefore might miss something important. In quantum physics and information theory, this relates to the observer effect and the importance of acknowledging the perspective influence of everyone in a scene.</p> 
<p>&#8220;In top-down hierarchies, the opposite occurs, yet the people on the front lines are the ones dealing with the evolutionary changes going on around them and are the best source of ideas for solving tactical issues. You wouldn&#8217;t need &#8220;change management&#8221; if you made continuous improvements at the functional level the responsibility of every individual and team cluster. Yet Pixar makes incredible films in this manner, so you could certainly accomplish it anywhere, even in a supermarket! In fact, some of Whole Foods&#8217; leadership practices are fractal &#8212; in-store teams make decisions about products and their placement, based on their observations of customer patterns.</p>

<p>&#8220;Most start-ups are fractal in their nature, especially those that have exciting visions and get everyone on the same page with collective purpose, goals, and objectives. Most investors, however, are bought into the conventional org chart; when the company devolves into top-down, the turnover begins. That&#8217;s because of the internal competition that emerges in top-down organizations. The perception is that there&#8217;s only so much room at the top. At each level of management, the competition increases as cooperation decreases.  Thus are created the ubiquitous &#8220;silos&#8221; of information that thwart collaboration and encourage redundant, wasteful business practices.</p>

<p>&#8220;Managers are supposedly promoted because of their ability to outperform others and not because of an intention to provide inspiration, guidance, and mentoring to their staff, nor are they openly rewarded for this behavior, even though it usually produces a healthier bottom line. The usual way of rewarding based upon meeting financial goals and managing budgets keeps the focus on short-term financial results only, whereas continuous improvement leadership by frontline staff creates more long-term successes.</p>

<p>&#8220;When managers don&#8217;t mentor staff, focusing only upon numbers and bossing people around, it leads to an illusion of control, of which there&#8217;s no such thing. In these situations, they begin to feel they must continually prove their worthiness and so defend their territories against possibly brilliant staff working &#8220;beneath&#8221; them. This is a systemic issue, not a personality quirk, though some personalities are more susceptible than others. In most companies, the idea of climbing over others on your way to the top and throwing people who get in your way under buses is de rigueur. The top-down hierarchy was designed to manage industrial-age processes, not information-age challenges. You didn&#8217;t want the door guy getting creative when attaching the door. Nor did he need to collaborate with the bumper dude. The information age is vastly different. Each scene we&#8217;re in presents new circumstances and opportunities.</p>

<p>&#8220;Pixar claims they have a meritocracy. This is a good description of the atmosphere that emerges in fractal organizations. Google was likely more fractal in the beginning, before they brought in managers trained in top-down hierarchies and engrained with the accompanying behavioral patterns, such as knee-jerk resistance to ideas they haven&#8217;t thought of themselves. Not everyone is like this, of course, but usually those who aren&#8217;t have had confident mentors themselves who encouraged creative participation. In Catmull&#8217;s <span class="caps">HBR </span>article, he says they insist that everyone in the company contribute ideas, across all functions and levels, or they will perish. Interestingly, he mentions the difficulty in getting new hires to feel comfortable with this process. This results from cultural-level systems that keep people in competition rather than cooperation. Though Catmull tells readers what they do at Pixar and why, he doesn&#8217;t instruct on how to make the organizational changes that enable this approach to creativity.&#8221;</p>

<p>I guess that last part is Janna&#8217;s job.</p>

<p>All this fractal stuff is interesting, but I&#8217;m guessing it is also difficult to implement, because it may describe Pixar but it <span class="caps">DOESN&#8217;T </span>accurately describe Apple &#8212; the other place where Steve Jobs is king.  Still, Apple&#8217;s product success shows that something is being transferred from one company to the other.  It&#8217;s just that at Apple, Steve Jobs can&#8217;t make himself stay out of the way.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080917_005420.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080917_005420.html</guid>
			<category></category>
			<pubDate>Wed, 17 Sep 2008 18:29:44 -0500</pubDate>
		</item>
				<item>
			<title>Fire Your Boss</title>
			<description><![CDATA[<p>This week marks the seventh anniversary of the 9/11 terrorist attacks in the <span class="caps">U.S. </span> This week is also a time when the world economy is under stress comparable or greater to that imposed seven years ago.  Whatever you are feeling in your wallet, I can&#8217;t overemphasize the impact the current global credit crunch is having on our economy and that of other nations, including Germany and Japan.  We&#8217;re in a mess &#8212; one that is at least <span class="caps">TWO YEARS </span>from being resolved no matter who is the President.  This matters to a technology columnist because it is something the technology community will eventually have to address, just as we did 9/11.  I think some coping skills are in order.</p>

<p>First let&#8217;s take a look at a small part of my column from September 13, 2001 &#8212; a column that wasn&#8217;t especially popular with readers at the time, but I think stands up pretty well with time:</p>

<p>&#8221; &#8216;To a man with a hammer, everything looks like a nail,&#8217; wrote Mark Twain. In the current context this means that the organizations charged with reacting to this catastrophe will do so by doing what they have always done, only more of it. Congress, which controls the budget and passes laws, will want to pass laws and to allocate more money, lots of money, forgetting completely about any campaign promises. The military, which is the nation&#8217;s enforcer, will want to use force, if only they can find a foe. The intelligence community, which gathers information, will want to be even more energetic in that gathering, no matter what the cost to the privacy of the millions of us who aren&#8217;t thinking of terrorist acts. And agencies like the Federal Aviation Administration, which regulate, will want to create more stringent regulations. Now here is an important point to be remembered: All these parties will want to do these things <span class="caps">WHETHER THEY ARE WARRANTED</span> OR <span class="caps">USEFUL</span> OR <span class="caps">NOT.</span>&#8221;</p>

<p>In 2008 we&#8217;re facing cuts in IT that are prompted by economic decline.  Many of the IT shops I talk to are in denial about this.  Many more, while not in denial, are making bad decisions.  I think this is a good opportunity to do some housecleaning that probably should have been done years ago.  If you have to cut your budget by 10 percent, where do you cut?  What if you have to cut by 30 percent?</p>

<p>As I have written before, one of the great problems in IT management is that the big bosses typically haven&#8217;t a clue what is happening, what is needed to happen, and what it all should cost.  There is a role for trust here, but if the Big Guy is signing off on a budget he can&#8217;t even read, much less understand, well something is wrong.  Some IT departments like this, of course, just like my students liked it when class had to be cancelled (they liked getting <span class="caps">LESS </span>for their money), but in tough times, facing reality and speaking the truth is usually the best course.</p>

<p>Because power in IT organizations tends to be based on head count, preserving jobs takes a priority.  And when jobs have to be eliminated, they tend to come off the bottom of the organization when they should more logically come off the top &#8212; or at least from near the top.  A tech who directly helps users is more important than a manager who can&#8217;t manage.  This is especially true if that manager is making 2-3 times as much as the tech.</p>

<p>If your boss doesn&#8217;t understand your job enough to describe it in technical detail, that boss is in the wrong job.  </p>

<p>If you are managing an IT shop and can&#8217;t write the code to render &#8220;hello world&#8221; in C, html, php, and pull &#8220;hello world&#8221; from a MySQL database using a perl script, then <span class="caps">YOU </span>are in the wrong job.</p>

<p>I should point out that these latter tasks can be copied and pasted straight from properly composed Google queries.  They aren&#8217;t a test of programming knowledge at all, just of the ability to use the Internet.  Yet many technical managers will fail and should get the boot as a result.  You can&#8217;t manage what you can&#8217;t understand.</p>

<p>Think about whole projects that can be chopped, too.  What&#8217;s really needed, after all?  That knowledge is in your organization, though often not where it is available to the decision makers.  The essence of efficiency is doing only the parts that are absolutely needed and almost every shop has at least one project that everyone except the big boss knows is either pointless or hopeless.  Cut it.</p>

<p>One can argue, of course, that <span class="caps">MORE IT, </span>not less, is in order, and maybe that argument will work.  But make it only if it is true.</p>

<p>I think a good argument can be made for embracing cloud computing, even to the extent of eliminating data centers and facilities.  We&#8217;re very close to the point where relatively few organizations really ought to have their own data centers.  </p>

<p>This could also be a good time to embrace open source tools.  Yes, there is a learning curve, but the price is right and I can argue that open source quality is substantially better.</p>

<p>Oh, and cancel those contracts with Gartner, Forrester, <span class="caps">IDC, </span>etc.  You&#8217;ll feel better in the morning.</p>

<p>Some folks who <span class="caps">WON&#8217;T </span>feel better in the morning are the next class of <span class="caps">IBM </span>employees to see their jobs moved overseas.  I understand that there is a new round of cuts coming in October and it will be different from the &#8220;death by a thousand cuts&#8221; that has been happening for the past year.  The technique is the same, of course &#8212; cutting unneeded workers in the <span class="caps">U.S. </span>while suddenly needing virtually identical (if younger and cheaper) workers in India and Argentina.  It&#8217;s pure coincidence.  Yeah, right.</p>

<p>If you were disappointed with Apple&#8217;s product announcements this week, take cheer from knowing that more announcements are coming, including new MacBooks and iMacs.  You don&#8217;t have to take my word for this &#8212; just look at the closeouts Apple is offering on current models.  Christmas is still the most important quarter for Apple so they won&#8217;t let these announcements wait too long.  I just wonder how they slipped out of this week&#8217;s event.</p>

<p>And finally, I am surprised to admit that the latest version of 64-bit Windows Vista seems to be running pretty darned well on my desktop.  No driver problems, 32- and 64-bit apps seem to be running well &#8212; why hasn&#8217;t Microsoft been shouting about this?  32-bit Vista still sucks, of course.</p>]]></description>
			<link>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080911_005419.html</link>
			<guid>http://www.pbs.org/cringely/pulpit/2008/pulpit_20080911_005419.html</guid>
			<category></category>
			<pubDate>Thu, 11 Sep 2008 17:54:05 -0500</pubDate>
		</item>
		
	</channel>
</rss>

